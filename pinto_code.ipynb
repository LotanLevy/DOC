{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pinto_code.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmrK7HSK7ruRrr6Z1tIyci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LotanLevy/DOC/blob/master/pinto_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EpcrZ0KE25J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow-gpu==2.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecouf20wMZcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eed632a4-df61-4ff3-ac0e-5bccbf095b7b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygOL2asKHd8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications import mobilenet_v2, vgg16\n",
        "\n",
        "\n",
        "def resize(x, size=96):\n",
        "    x_out = []\n",
        "    for i in range(len(x)):\n",
        "      img = x[i]\n",
        "      if img.shape[-1] == 1:\n",
        "        img = cv2.cvtColor(x[i], cv2.COLOR_GRAY2RGB)\n",
        "      img = cv2.resize(img,dsize=(size,size))\n",
        "      x_out.append(img)\n",
        "\n",
        "    return np.array(x_out)\n",
        "\n",
        "def mobilenet_v2_preprocessing(input_data):\n",
        "  return input_data.astype('float32')/ 255\n",
        "\n",
        "\n",
        "def vgg_preprocessing(input_data):\n",
        "  # return vgg16.preprocess_input(resize(np.copy(input_data).astype('float32'), size=size))\n",
        "    return vgg16.preprocess_input(np.copy(input_data.astype('float32')))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHo-SyQ4cVZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np \n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class DataIter:\n",
        "  def __init__(self, data, labels, batch_size, classes_num, shuffle=False, preprocess_func=lambda x:x):\n",
        "    if labels is not None:\n",
        "        assert(len(data) == len(labels))\n",
        "        self.labels = np.array(labels)\n",
        "    else:\n",
        "      self.labels = None\n",
        "\n",
        "    self.preprocess_func = preprocess_func\n",
        "\n",
        "    self.data = data\n",
        "    self.classes_num = classes_num\n",
        "    self.batch_size = batch_size\n",
        "    self.indices = np.arange(len(self.data)).astype(np.int)\n",
        "    self.shuffle = shuffle\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(self.indices)\n",
        "    self.cur_idx = 0\n",
        "\n",
        "  def next(self):\n",
        "    relevant_indices = self.indices[self.cur_idx: self.cur_idx + self.batch_size]\n",
        "    self.cur_idx += self.batch_size\n",
        "    images = self.data[relevant_indices]\n",
        "    if self.labels is not None:\n",
        "      labels = self.labels[relevant_indices]\n",
        "      labels = tf.keras.utils.to_categorical(labels, num_classes=self.classes_num)\n",
        "    else:\n",
        "      labels = None\n",
        "    return resize(self.preprocess_func(images), size), labels\n",
        "\n",
        "  def get_all_data(self, size=None):\n",
        "    if size is None:\n",
        "      size = len(self.data)\n",
        "    return resize(self.preprocess_func(self.data[:size]), size), None\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "\n",
        "class DirIter:\n",
        "    def __init__(self, paths, labels, batch_size, input_size, classes_num, shuffle=False, preprocess_func=lambda x:x):\n",
        "      if labels is not None:\n",
        "        assert(len(paths) == len(labels))\n",
        "        self.labels = np.array(labels)\n",
        "      else:\n",
        "        self.labels = None\n",
        "      self.preprocess_func = preprocess_func\n",
        "\n",
        "      self.paths = paths\n",
        "      self.classes_num = classes_num\n",
        "      self.batch_size = batch_size\n",
        "      self.indices = np.arange(len(self.paths)).astype(np.int)\n",
        "      self.input_size = input_size\n",
        "      self.shuffle = shuffle\n",
        "      self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.paths)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "      if self.shuffle and len(self.indices) > 0:\n",
        "        np.random.shuffle(self.indices)\n",
        "      self.cur_idx = 0\n",
        "\n",
        "    def load_img(self, image_path):\n",
        "        image = Image.open(image_path, 'r')\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "        image = image.resize(self.input_size, Image.NEAREST)\n",
        "        image = np.array(image).astype(np.float32)\n",
        "        return np.expand_dims(image, axis=0)\n",
        "\n",
        "    def next(self):\n",
        "        relevant_indices = self.indices[self.cur_idx: self.cur_idx + self.batch_size]\n",
        "        self.cur_idx += self.batch_size\n",
        "        images = []\n",
        "        images = np.concatenate([self.load_img(self.paths[i]) for i in relevant_indices])\n",
        "        if self.labels is not None:\n",
        "          labels = self.labels[relevant_indices]\n",
        "          labels = tf.keras.utils.to_categorical(labels, num_classes=self.classes_num)\n",
        "        else:\n",
        "          labels = None\n",
        "        return self.preprocess_func(images), labels\n",
        "\n",
        "    def get_all_data(self, size=None):\n",
        "      if size is None:\n",
        "        size = len(self.paths)\n",
        "      relevant_paths = [self.paths[i] for i in relevant_indices[:size]]\n",
        "      images = np.concatenate([self.load_img(path) for path in relevant_paths])\n",
        "      return self.preprocess_func(images), relevant_paths\n",
        "\n",
        "    def has_next(self):\n",
        "      return self.cur_idx + self.batch_size < len(self.indices)\n",
        "\n",
        "    def set_cls2label_map(self, map):\n",
        "        self.cls2label = map\n",
        "\n",
        "\n",
        "def get_iterators_by_root_dir(root_dir, batch_size, input_size, split_val, classes_num, shuffle=False):\n",
        "    dirs = os.listdir(root_dir)\n",
        "    length = len(max(dirs, key=len))\n",
        "\n",
        "    for dir in dirs: # Handle the sort problem pads the clas num with '0'\n",
        "        if len(dir) < length:\n",
        "          zeros = \"0\" * (length - len(dir))\n",
        "          new_name = zeros + dir\n",
        "\n",
        "          os.rename(os.path.join(root_dir, dir), os.path.join(root_dir, new_name))\n",
        "          print(\"old {}, new {}\".format(dir, new_name))\n",
        "\n",
        "    paths = []\n",
        "    labels = []\n",
        "    cls2label = dict()\n",
        "    label_idx = 0\n",
        "    for sub_dir in sorted(os.listdir(root_dir)):\n",
        "\n",
        "        full_path = os.path.join(root_dir, sub_dir)\n",
        "        if not os.path.isdir(full_path):\n",
        "            continue\n",
        "        cls2label[sub_dir] = label_idx\n",
        "        for file in os.listdir(full_path):\n",
        "            paths.append(os.path.join(full_path, file))\n",
        "            labels.append(label_idx)\n",
        "        label_idx += 1\n",
        "\n",
        "    print(cls2label)\n",
        "\n",
        "\n",
        "    assert len(paths) == len(labels)\n",
        "    if len(cls2label) != classes_num:\n",
        "        print(\"classes in directory doesn't match classes_num\")\n",
        "\n",
        "    if split_val > 0:\n",
        "      X_train, X_test, y_train, y_test = train_test_split(paths, labels, test_size=split_val, shuffle=shuffle)\n",
        "    else:\n",
        "      X_train, X_test, y_train, y_test = paths, [], labels, []\n",
        "\n",
        "    train_iter = DirIter(X_train, y_train, batch_size, input_size, classes_num, shuffle=True)\n",
        "    val_iter = DirIter(X_test, y_test, batch_size, input_size, classes_num, shuffle=(True))\n",
        "\n",
        "    train_iter.set_cls2label_map(cls2label)\n",
        "    val_iter.set_cls2label_map(cls2label)\n",
        "    return train_iter, val_iter\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbTH9ooTDivN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "def get_fmnist_data_loaders():\n",
        "  # dataset\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "  #Splitting the into subsets data\n",
        "  x_train_s, x_test_s, x_test_b = [], [], []\n",
        "  x_ref, y_ref = [], []\n",
        "\n",
        "  x_train_shape = x_train.shape\n",
        "  #train data\n",
        "  for i in range(len(x_train)):\n",
        "      if y_train[i] == 7: #Sneakers is 7\n",
        "          temp = x_train[i]\n",
        "          x_train_s.append(temp.reshape((x_train_shape[1:])))\n",
        "      else:\n",
        "          temp = x_train[i]\n",
        "          x_ref.append(temp.reshape((x_train_shape[1:])))\n",
        "          y_ref.append(y_train[i])\n",
        "\n",
        "  x_ref, y_ref = np.array(x_ref), np.array(y_ref)\n",
        "\n",
        "  #test data\n",
        "  for i in range(len(x_test)):\n",
        "      if y_test[i] == 7: #Sneakers is 7\n",
        "          temp = x_test[i,:,:,:]\n",
        "          x_test_s.append(temp.reshape((x_train_shape[1:])))\n",
        "\n",
        "      if y_test[i] == 9: #Boots is 9\n",
        "          temp = x_test[i,:,:,:]\n",
        "          x_test_b.append(temp.reshape((x_train_shape[1:])))\n",
        "\n",
        "  #tdata loaders\n",
        "  train_s_loader = DataIter(np.array(x_train_s), None, batchsize, classes, shuffle=True, preprocess_func=preprocessing_func)\n",
        "  ref_loader = DataIter(np.array(x_ref), np.array(y_ref), batchsize, classes, shuffle=True, preprocess_func=preprocessing_func)\n",
        "\n",
        "  test_s_loader = DataIter(np.array(x_test_s), None, batchsize, classes, preprocess_func=preprocessing_func)\n",
        "  test_b_loader = DataIter(np.array(x_test_b), None, batchsize, classes, preprocess_func=preprocessing_func)\n",
        "  return  train_s_loader, ref_loader, test_s_loader, test_b_loader\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li9xDsADqH9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_imagenet_clatech_loaders(ref_path, tar_path, alien_path, batchsize, input_size, split_val, cls_num, shuffle=False):\n",
        "  ref_loader, i1 = get_iterators_by_root_dir(ref_path, batchsize, input_size, 0, cls_num, shuffle=shuffle)\n",
        "  train_s_loader, test_s_loader = get_iterators_by_root_dir(tar_path, batchsize, input_size, split_val, cls_num, shuffle=shuffle)\n",
        "  test_alien_loader, i2 = get_iterators_by_root_dir(alien_path, batchsize, input_size, 0, cls_num, shuffle=shuffle)\n",
        "  print(len(ref_loader), len(i1))\n",
        "  print(len(train_s_loader), len(test_s_loader))\n",
        "  print(len(test_alien_loader), len(i2))\n",
        "\n",
        "  return  train_s_loader, ref_loader, test_s_loader, test_alien_loader\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjiyMA8pu6gY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "a9eb497d-8498-4a2e-d5c0-c9fdf73635f8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "output_path = \"/content/drive/My Drive/Colab Notebooks/affordances/experiments/experiment_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "if os.path.exists(output_path):\n",
        "  os.makedirs(output_path)\n",
        "ckpt_path = os.path.join(output_path, \"ckpts\")\n",
        "if os.path.exists(ckpt_path):\n",
        "  os.makedirs(output_path)\n",
        "\n",
        "epochs_log_dir = os.path.join(os.path.join(output_path, \"epochs_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
        "batchs_log_dir = os.path.join(os.path.join(output_path, \"batchs_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# MobileNetV2\n",
        "# size = 96\n",
        "\n",
        "# preprocessing_func = mobilenet_v2_preprocessing\n",
        "# first_trained_layer_name = \"block_13_expand\"\n",
        "# alpha = 0.5 #for MobileNetV2\n",
        "# network_constractor = lambda : tf.keras.applications.MobileNetV2(include_top=True, input_shape=(size, size, 3), alpha=alpha, weights='imagenet')\n",
        "\n",
        "\n",
        "# VGG16\n",
        "size = 224\n",
        "preprocessing_func = vgg_preprocessing\n",
        "first_trained_layer_name =  \"block5_conv1\"\n",
        "network_constractor = lambda : tf.keras.applications.VGG16(include_top=True, input_shape=(size, size, 3), weights='imagenet')\n",
        "test_size = 200\n",
        "\n",
        "\n",
        "\n",
        "lambda_ = 0.1 #for compact loss\n",
        "\n",
        "# fmnist data\n",
        "# classes = 10\n",
        "# batchsize = 2\n",
        "\n",
        "# train_s_loader, ref_loader, test_s_loader, test_b_loader = get_fmnist_data_loaders()\n",
        "\n",
        "# # imagenet clatech data\n",
        "REFERENCE_PATH = \"/content/drive/My Drive/Colab Notebooks/affordances/datasets/imagenet_val_splitted\"\n",
        "TARGET_PATH = \"/content/drive/My Drive/Colab Notebooks/affordances/datasets/knife_target_data\"\n",
        "ALIEN_PATH = \"/content/drive/My Drive/Colab Notebooks/affordances/datasets/tovisualize\"\n",
        "classes = 1000\n",
        "batchsize = 2\n",
        "split_val = 0.2\n",
        "\n",
        "train_s_loader, ref_loader, test_s_loader, test_b_loader = get_imagenet_clatech_loaders(REFERENCE_PATH, TARGET_PATH, ALIEN_PATH, batchsize, (size,size), split_val, classes, shuffle=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0000000000000000': 0, '0000000000000001': 1, '0000000000000002': 2, '0000000000000003': 3, '0000000000000004': 4, '0000000000000005': 5, '0000000000000006': 6, '0000000000000007': 7, '0000000000000008': 8, '0000000000000009': 9, '0000000000000010': 10, '0000000000000011': 11, '0000000000000012': 12, '0000000000000013': 13, '0000000000000014': 14, '0000000000000015': 15, '0000000000000016': 16, '0000000000000017': 17, '0000000000000018': 18, '0000000000000019': 19, '0000000000000020': 20, '0000000000000021': 21, '0000000000000022': 22, '0000000000000023': 23, '0000000000000024': 24, '0000000000000025': 25, '0000000000000026': 26, '0000000000000027': 27, '0000000000000028': 28, '0000000000000029': 29, '0000000000000030': 30, '0000000000000031': 31, '0000000000000032': 32, '0000000000000033': 33, '0000000000000034': 34, '0000000000000035': 35, '0000000000000036': 36, '0000000000000037': 37, '0000000000000038': 38, '0000000000000039': 39, '0000000000000040': 40, '0000000000000041': 41, '0000000000000042': 42, '0000000000000043': 43, '0000000000000044': 44, '0000000000000045': 45, '0000000000000046': 46, '0000000000000047': 47, '0000000000000048': 48, '0000000000000049': 49, '0000000000000050': 50, '0000000000000051': 51, '0000000000000052': 52, '0000000000000053': 53, '0000000000000054': 54, '0000000000000055': 55, '0000000000000056': 56, '0000000000000057': 57, '0000000000000058': 58, '0000000000000059': 59, '0000000000000060': 60, '0000000000000061': 61, '0000000000000062': 62, '0000000000000063': 63, '0000000000000064': 64, '0000000000000065': 65, '0000000000000066': 66, '0000000000000067': 67, '0000000000000068': 68, '0000000000000069': 69, '0000000000000070': 70, '0000000000000071': 71, '0000000000000072': 72, '0000000000000073': 73, '0000000000000074': 74, '0000000000000075': 75, '0000000000000076': 76, '0000000000000077': 77, '0000000000000078': 78, '0000000000000079': 79, '0000000000000080': 80, '0000000000000081': 81, '0000000000000082': 82, '0000000000000083': 83, '0000000000000084': 84, '0000000000000085': 85, '0000000000000086': 86, '0000000000000087': 87, '0000000000000088': 88, '0000000000000089': 89, '0000000000000090': 90, '0000000000000091': 91, '0000000000000092': 92, '0000000000000093': 93, '0000000000000094': 94, '0000000000000095': 95, '0000000000000096': 96, '0000000000000097': 97, '0000000000000098': 98, '0000000000000099': 99, '0000000000000100': 100, '0000000000000101': 101, '0000000000000102': 102, '0000000000000103': 103, '0000000000000104': 104, '0000000000000105': 105, '0000000000000106': 106, '0000000000000107': 107, '0000000000000108': 108, '0000000000000109': 109, '0000000000000110': 110, '0000000000000111': 111, '0000000000000112': 112, '0000000000000113': 113, '0000000000000114': 114, '0000000000000115': 115, '0000000000000116': 116, '0000000000000117': 117, '0000000000000118': 118, '0000000000000119': 119, '0000000000000120': 120, '0000000000000121': 121, '0000000000000122': 122, '0000000000000123': 123, '0000000000000124': 124, '0000000000000125': 125, '0000000000000126': 126, '0000000000000127': 127, '0000000000000128': 128, '0000000000000129': 129, '0000000000000130': 130, '0000000000000131': 131, '0000000000000132': 132, '0000000000000133': 133, '0000000000000134': 134, '0000000000000135': 135, '0000000000000136': 136, '0000000000000137': 137, '0000000000000138': 138, '0000000000000139': 139, '0000000000000140': 140, '0000000000000141': 141, '0000000000000142': 142, '0000000000000143': 143, '0000000000000144': 144, '0000000000000145': 145, '0000000000000146': 146, '0000000000000147': 147, '0000000000000148': 148, '0000000000000149': 149, '0000000000000150': 150, '0000000000000151': 151, '0000000000000152': 152, '0000000000000153': 153, '0000000000000154': 154, '0000000000000155': 155, '0000000000000156': 156, '0000000000000157': 157, '0000000000000158': 158, '0000000000000159': 159, '0000000000000160': 160, '0000000000000161': 161, '0000000000000162': 162, '0000000000000163': 163, '0000000000000164': 164, '0000000000000165': 165, '0000000000000166': 166, '0000000000000167': 167, '0000000000000168': 168, '0000000000000169': 169, '0000000000000170': 170, '0000000000000171': 171, '0000000000000172': 172, '0000000000000173': 173, '0000000000000174': 174, '0000000000000175': 175, '0000000000000176': 176, '0000000000000177': 177, '0000000000000178': 178, '0000000000000179': 179, '0000000000000180': 180, '0000000000000181': 181, '0000000000000182': 182, '0000000000000183': 183, '0000000000000184': 184, '0000000000000185': 185, '0000000000000186': 186, '0000000000000187': 187, '0000000000000188': 188, '0000000000000189': 189, '0000000000000190': 190, '0000000000000191': 191, '0000000000000192': 192, '0000000000000193': 193, '0000000000000194': 194, '0000000000000195': 195, '0000000000000196': 196, '0000000000000197': 197, '0000000000000198': 198, '0000000000000199': 199, '0000000000000200': 200, '0000000000000201': 201, '0000000000000202': 202, '0000000000000203': 203, '0000000000000204': 204, '0000000000000205': 205, '0000000000000206': 206, '0000000000000207': 207, '0000000000000208': 208, '0000000000000209': 209, '0000000000000210': 210, '0000000000000211': 211, '0000000000000212': 212, '0000000000000213': 213, '0000000000000214': 214, '0000000000000215': 215, '0000000000000216': 216, '0000000000000217': 217, '0000000000000218': 218, '0000000000000219': 219, '0000000000000220': 220, '0000000000000221': 221, '0000000000000222': 222, '0000000000000223': 223, '0000000000000224': 224, '0000000000000225': 225, '0000000000000226': 226, '0000000000000227': 227, '0000000000000228': 228, '0000000000000229': 229, '0000000000000230': 230, '0000000000000231': 231, '0000000000000232': 232, '0000000000000233': 233, '0000000000000234': 234, '0000000000000235': 235, '0000000000000236': 236, '0000000000000237': 237, '0000000000000238': 238, '0000000000000239': 239, '0000000000000240': 240, '0000000000000241': 241, '0000000000000242': 242, '0000000000000243': 243, '0000000000000244': 244, '0000000000000245': 245, '0000000000000246': 246, '0000000000000247': 247, '0000000000000248': 248, '0000000000000249': 249, '0000000000000250': 250, '0000000000000251': 251, '0000000000000252': 252, '0000000000000253': 253, '0000000000000254': 254, '0000000000000255': 255, '0000000000000256': 256, '0000000000000257': 257, '0000000000000258': 258, '0000000000000259': 259, '0000000000000260': 260, '0000000000000261': 261, '0000000000000262': 262, '0000000000000263': 263, '0000000000000264': 264, '0000000000000265': 265, '0000000000000266': 266, '0000000000000267': 267, '0000000000000268': 268, '0000000000000269': 269, '0000000000000270': 270, '0000000000000271': 271, '0000000000000272': 272, '0000000000000273': 273, '0000000000000274': 274, '0000000000000275': 275, '0000000000000276': 276, '0000000000000277': 277, '0000000000000278': 278, '0000000000000279': 279, '0000000000000280': 280, '0000000000000281': 281, '0000000000000282': 282, '0000000000000283': 283, '0000000000000284': 284, '0000000000000285': 285, '0000000000000286': 286, '0000000000000287': 287, '0000000000000288': 288, '0000000000000289': 289, '0000000000000290': 290, '0000000000000291': 291, '0000000000000292': 292, '0000000000000293': 293, '0000000000000294': 294, '0000000000000295': 295, '0000000000000296': 296, '0000000000000297': 297, '0000000000000298': 298, '0000000000000299': 299, '0000000000000300': 300, '0000000000000301': 301, '0000000000000302': 302, '0000000000000303': 303, '0000000000000304': 304, '0000000000000305': 305, '0000000000000306': 306, '0000000000000307': 307, '0000000000000308': 308, '0000000000000309': 309, '0000000000000310': 310, '0000000000000311': 311, '0000000000000312': 312, '0000000000000313': 313, '0000000000000314': 314, '0000000000000315': 315, '0000000000000316': 316, '0000000000000317': 317, '0000000000000318': 318, '0000000000000319': 319, '0000000000000320': 320, '0000000000000321': 321, '0000000000000322': 322, '0000000000000323': 323, '0000000000000324': 324, '0000000000000325': 325, '0000000000000326': 326, '0000000000000327': 327, '0000000000000328': 328, '0000000000000329': 329, '0000000000000330': 330, '0000000000000331': 331, '0000000000000332': 332, '0000000000000333': 333, '0000000000000334': 334, '0000000000000335': 335, '0000000000000336': 336, '0000000000000337': 337, '0000000000000338': 338, '0000000000000339': 339, '0000000000000340': 340, '0000000000000341': 341, '0000000000000342': 342, '0000000000000343': 343, '0000000000000344': 344, '0000000000000345': 345, '0000000000000346': 346, '0000000000000347': 347, '0000000000000348': 348, '0000000000000349': 349, '0000000000000350': 350, '0000000000000351': 351, '0000000000000352': 352, '0000000000000353': 353, '0000000000000354': 354, '0000000000000355': 355, '0000000000000356': 356, '0000000000000357': 357, '0000000000000358': 358, '0000000000000359': 359, '0000000000000360': 360, '0000000000000361': 361, '0000000000000362': 362, '0000000000000363': 363, '0000000000000364': 364, '0000000000000365': 365, '0000000000000366': 366, '0000000000000367': 367, '0000000000000368': 368, '0000000000000369': 369, '0000000000000370': 370, '0000000000000371': 371, '0000000000000372': 372, '0000000000000373': 373, '0000000000000374': 374, '0000000000000375': 375, '0000000000000376': 376, '0000000000000377': 377, '0000000000000378': 378, '0000000000000379': 379, '0000000000000380': 380, '0000000000000381': 381, '0000000000000382': 382, '0000000000000383': 383, '0000000000000384': 384, '0000000000000385': 385, '0000000000000386': 386, '0000000000000387': 387, '0000000000000388': 388, '0000000000000389': 389, '0000000000000390': 390, '0000000000000391': 391, '0000000000000392': 392, '0000000000000393': 393, '0000000000000394': 394, '0000000000000395': 395, '0000000000000396': 396, '0000000000000397': 397, '0000000000000398': 398, '0000000000000399': 399, '0000000000000400': 400, '0000000000000401': 401, '0000000000000402': 402, '0000000000000403': 403, '0000000000000404': 404, '0000000000000405': 405, '0000000000000406': 406, '0000000000000407': 407, '0000000000000408': 408, '0000000000000409': 409, '0000000000000410': 410, '0000000000000411': 411, '0000000000000412': 412, '0000000000000413': 413, '0000000000000414': 414, '0000000000000415': 415, '0000000000000416': 416, '0000000000000417': 417, '0000000000000418': 418, '0000000000000419': 419, '0000000000000420': 420, '0000000000000421': 421, '0000000000000422': 422, '0000000000000423': 423, '0000000000000424': 424, '0000000000000425': 425, '0000000000000426': 426, '0000000000000427': 427, '0000000000000428': 428, '0000000000000429': 429, '0000000000000430': 430, '0000000000000431': 431, '0000000000000432': 432, '0000000000000433': 433, '0000000000000434': 434, '0000000000000435': 435, '0000000000000436': 436, '0000000000000437': 437, '0000000000000438': 438, '0000000000000439': 439, '0000000000000440': 440, '0000000000000441': 441, '0000000000000442': 442, '0000000000000443': 443, '0000000000000444': 444, '0000000000000445': 445, '0000000000000446': 446, '0000000000000447': 447, '0000000000000448': 448, '0000000000000449': 449, '0000000000000450': 450, '0000000000000451': 451, '0000000000000452': 452, '0000000000000453': 453, '0000000000000454': 454, '0000000000000455': 455, '0000000000000456': 456, '0000000000000457': 457, '0000000000000458': 458, '0000000000000459': 459, '0000000000000460': 460, '0000000000000461': 461, '0000000000000462': 462, '0000000000000463': 463, '0000000000000464': 464, '0000000000000465': 465, '0000000000000466': 466, '0000000000000467': 467, '0000000000000468': 468, '0000000000000469': 469, '0000000000000470': 470, '0000000000000471': 471, '0000000000000472': 472, '0000000000000473': 473, '0000000000000474': 474, '0000000000000475': 475, '0000000000000476': 476, '0000000000000477': 477, '0000000000000478': 478, '0000000000000479': 479, '0000000000000480': 480, '0000000000000481': 481, '0000000000000482': 482, '0000000000000483': 483, '0000000000000484': 484, '0000000000000485': 485, '0000000000000486': 486, '0000000000000487': 487, '0000000000000488': 488, '0000000000000489': 489, '0000000000000490': 490, '0000000000000491': 491, '0000000000000492': 492, '0000000000000493': 493, '0000000000000494': 494, '0000000000000495': 495, '0000000000000496': 496, '0000000000000497': 497, '0000000000000498': 498, '0000000000000499': 499, '0000000000000500': 500, '0000000000000501': 501, '0000000000000502': 502, '0000000000000503': 503, '0000000000000504': 504, '0000000000000505': 505, '0000000000000506': 506, '0000000000000507': 507, '0000000000000508': 508, '0000000000000509': 509, '0000000000000510': 510, '0000000000000511': 511, '0000000000000512': 512, '0000000000000513': 513, '0000000000000514': 514, '0000000000000515': 515, '0000000000000516': 516, '0000000000000517': 517, '0000000000000518': 518, '0000000000000519': 519, '0000000000000520': 520, '0000000000000521': 521, '0000000000000522': 522, '0000000000000523': 523, '0000000000000524': 524, '0000000000000525': 525, '0000000000000526': 526, '0000000000000527': 527, '0000000000000528': 528, '0000000000000529': 529, '0000000000000530': 530, '0000000000000531': 531, '0000000000000532': 532, '0000000000000533': 533, '0000000000000534': 534, '0000000000000535': 535, '0000000000000536': 536, '0000000000000537': 537, '0000000000000538': 538, '0000000000000539': 539, '0000000000000540': 540, '0000000000000541': 541, '0000000000000542': 542, '0000000000000543': 543, '0000000000000544': 544, '0000000000000545': 545, '0000000000000546': 546, '0000000000000547': 547, '0000000000000548': 548, '0000000000000549': 549, '0000000000000550': 550, '0000000000000551': 551, '0000000000000552': 552, '0000000000000553': 553, '0000000000000554': 554, '0000000000000555': 555, '0000000000000556': 556, '0000000000000557': 557, '0000000000000558': 558, '0000000000000559': 559, '0000000000000560': 560, '0000000000000561': 561, '0000000000000562': 562, '0000000000000563': 563, '0000000000000564': 564, '0000000000000565': 565, '0000000000000566': 566, '0000000000000567': 567, '0000000000000568': 568, '0000000000000569': 569, '0000000000000570': 570, '0000000000000571': 571, '0000000000000572': 572, '0000000000000573': 573, '0000000000000574': 574, '0000000000000575': 575, '0000000000000576': 576, '0000000000000577': 577, '0000000000000578': 578, '0000000000000579': 579, '0000000000000580': 580, '0000000000000581': 581, '0000000000000582': 582, '0000000000000583': 583, '0000000000000584': 584, '0000000000000585': 585, '0000000000000586': 586, '0000000000000587': 587, '0000000000000588': 588, '0000000000000589': 589, '0000000000000590': 590, '0000000000000591': 591, '0000000000000592': 592, '0000000000000593': 593, '0000000000000594': 594, '0000000000000595': 595, '0000000000000596': 596, '0000000000000597': 597, '0000000000000598': 598, '0000000000000599': 599, '0000000000000600': 600, '0000000000000601': 601, '0000000000000602': 602, '0000000000000603': 603, '0000000000000604': 604, '0000000000000605': 605, '0000000000000606': 606, '0000000000000607': 607, '0000000000000608': 608, '0000000000000609': 609, '0000000000000610': 610, '0000000000000611': 611, '0000000000000612': 612, '0000000000000613': 613, '0000000000000614': 614, '0000000000000615': 615, '0000000000000616': 616, '0000000000000617': 617, '0000000000000618': 618, '0000000000000619': 619, '0000000000000620': 620, '0000000000000621': 621, '0000000000000622': 622, '0000000000000623': 623, '0000000000000624': 624, '0000000000000625': 625, '0000000000000626': 626, '0000000000000627': 627, '0000000000000628': 628, '0000000000000629': 629, '0000000000000630': 630, '0000000000000631': 631, '0000000000000632': 632, '0000000000000633': 633, '0000000000000634': 634, '0000000000000635': 635, '0000000000000636': 636, '0000000000000637': 637, '0000000000000638': 638, '0000000000000639': 639, '0000000000000640': 640, '0000000000000641': 641, '0000000000000642': 642, '0000000000000643': 643, '0000000000000644': 644, '0000000000000645': 645, '0000000000000646': 646, '0000000000000647': 647, '0000000000000648': 648, '0000000000000649': 649, '0000000000000650': 650, '0000000000000651': 651, '0000000000000652': 652, '0000000000000653': 653, '0000000000000654': 654, '0000000000000655': 655, '0000000000000656': 656, '0000000000000657': 657, '0000000000000658': 658, '0000000000000659': 659, '0000000000000660': 660, '0000000000000661': 661, '0000000000000662': 662, '0000000000000663': 663, '0000000000000664': 664, '0000000000000665': 665, '0000000000000666': 666, '0000000000000667': 667, '0000000000000668': 668, '0000000000000669': 669, '0000000000000670': 670, '0000000000000671': 671, '0000000000000672': 672, '0000000000000673': 673, '0000000000000674': 674, '0000000000000675': 675, '0000000000000676': 676, '0000000000000677': 677, '0000000000000678': 678, '0000000000000679': 679, '0000000000000680': 680, '0000000000000681': 681, '0000000000000682': 682, '0000000000000683': 683, '0000000000000684': 684, '0000000000000685': 685, '0000000000000686': 686, '0000000000000687': 687, '0000000000000688': 688, '0000000000000689': 689, '0000000000000690': 690, '0000000000000691': 691, '0000000000000692': 692, '0000000000000693': 693, '0000000000000694': 694, '0000000000000695': 695, '0000000000000696': 696, '0000000000000697': 697, '0000000000000698': 698, '0000000000000699': 699, '0000000000000700': 700, '0000000000000701': 701, '0000000000000702': 702, '0000000000000703': 703, '0000000000000704': 704, '0000000000000705': 705, '0000000000000706': 706, '0000000000000707': 707, '0000000000000708': 708, '0000000000000709': 709, '0000000000000710': 710, '0000000000000711': 711, '0000000000000712': 712, '0000000000000713': 713, '0000000000000714': 714, '0000000000000715': 715, '0000000000000716': 716, '0000000000000717': 717, '0000000000000718': 718, '0000000000000719': 719, '0000000000000720': 720, '0000000000000721': 721, '0000000000000722': 722, '0000000000000723': 723, '0000000000000724': 724, '0000000000000725': 725, '0000000000000726': 726, '0000000000000727': 727, '0000000000000728': 728, '0000000000000729': 729, '0000000000000730': 730, '0000000000000731': 731, '0000000000000732': 732, '0000000000000733': 733, '0000000000000734': 734, '0000000000000735': 735, '0000000000000736': 736, '0000000000000737': 737, '0000000000000738': 738, '0000000000000739': 739, '0000000000000740': 740, '0000000000000741': 741, '0000000000000742': 742, '0000000000000743': 743, '0000000000000744': 744, '0000000000000745': 745, '0000000000000746': 746, '0000000000000747': 747, '0000000000000748': 748, '0000000000000749': 749, '0000000000000750': 750, '0000000000000751': 751, '0000000000000752': 752, '0000000000000753': 753, '0000000000000754': 754, '0000000000000755': 755, '0000000000000756': 756, '0000000000000757': 757, '0000000000000758': 758, '0000000000000759': 759, '0000000000000760': 760, '0000000000000761': 761, '0000000000000762': 762, '0000000000000763': 763, '0000000000000764': 764, '0000000000000765': 765, '0000000000000766': 766, '0000000000000767': 767, '0000000000000768': 768, '0000000000000769': 769, '0000000000000770': 770, '0000000000000771': 771, '0000000000000772': 772, '0000000000000773': 773, '0000000000000774': 774, '0000000000000775': 775, '0000000000000776': 776, '0000000000000777': 777, '0000000000000778': 778, '0000000000000779': 779, '0000000000000780': 780, '0000000000000781': 781, '0000000000000782': 782, '0000000000000783': 783, '0000000000000784': 784, '0000000000000785': 785, '0000000000000786': 786, '0000000000000787': 787, '0000000000000788': 788, '0000000000000789': 789, '0000000000000790': 790, '0000000000000791': 791, '0000000000000792': 792, '0000000000000793': 793, '0000000000000794': 794, '0000000000000795': 795, '0000000000000796': 796, '0000000000000797': 797, '0000000000000798': 798, '0000000000000799': 799, '0000000000000800': 800, '0000000000000801': 801, '0000000000000802': 802, '0000000000000803': 803, '0000000000000804': 804, '0000000000000805': 805, '0000000000000806': 806, '0000000000000807': 807, '0000000000000808': 808, '0000000000000809': 809, '0000000000000810': 810, '0000000000000811': 811, '0000000000000812': 812, '0000000000000813': 813, '0000000000000814': 814, '0000000000000815': 815, '0000000000000816': 816, '0000000000000817': 817, '0000000000000818': 818, '0000000000000819': 819, '0000000000000820': 820, '0000000000000821': 821, '0000000000000822': 822, '0000000000000823': 823, '0000000000000824': 824, '0000000000000825': 825, '0000000000000826': 826, '0000000000000827': 827, '0000000000000828': 828, '0000000000000829': 829, '0000000000000830': 830, '0000000000000831': 831, '0000000000000832': 832, '0000000000000833': 833, '0000000000000834': 834, '0000000000000835': 835, '0000000000000836': 836, '0000000000000837': 837, '0000000000000838': 838, '0000000000000839': 839, '0000000000000840': 840, '0000000000000841': 841, '0000000000000842': 842, '0000000000000843': 843, '0000000000000844': 844, '0000000000000845': 845, '0000000000000846': 846, '0000000000000847': 847, '0000000000000848': 848, '0000000000000849': 849, '0000000000000850': 850, '0000000000000851': 851, '0000000000000852': 852, '0000000000000853': 853, '0000000000000854': 854, '0000000000000855': 855, '0000000000000856': 856, '0000000000000857': 857, '0000000000000858': 858, '0000000000000859': 859, '0000000000000860': 860, '0000000000000861': 861, '0000000000000862': 862, '0000000000000863': 863, '0000000000000864': 864, '0000000000000865': 865, '0000000000000866': 866, '0000000000000867': 867, '0000000000000868': 868, '0000000000000869': 869, '0000000000000870': 870, '0000000000000871': 871, '0000000000000872': 872, '0000000000000873': 873, '0000000000000874': 874, '0000000000000875': 875, '0000000000000876': 876, '0000000000000877': 877, '0000000000000878': 878, '0000000000000879': 879, '0000000000000880': 880, '0000000000000881': 881, '0000000000000882': 882, '0000000000000883': 883, '0000000000000884': 884, '0000000000000885': 885, '0000000000000886': 886, '0000000000000887': 887, '0000000000000888': 888, '0000000000000889': 889, '0000000000000890': 890, '0000000000000891': 891, '0000000000000892': 892, '0000000000000893': 893, '0000000000000894': 894, '0000000000000895': 895, '0000000000000896': 896, '0000000000000897': 897, '0000000000000898': 898, '0000000000000899': 899, '0000000000000900': 900, '0000000000000901': 901, '0000000000000902': 902, '0000000000000903': 903, '0000000000000904': 904, '0000000000000905': 905, '0000000000000906': 906, '0000000000000907': 907, '0000000000000908': 908, '0000000000000909': 909, '0000000000000910': 910, '0000000000000911': 911, '0000000000000912': 912, '0000000000000913': 913, '0000000000000914': 914, '0000000000000915': 915, '0000000000000916': 916, '0000000000000917': 917, '0000000000000918': 918, '0000000000000919': 919, '0000000000000920': 920, '0000000000000921': 921, '0000000000000922': 922, '0000000000000923': 923, '0000000000000924': 924, '0000000000000925': 925, '0000000000000926': 926, '0000000000000927': 927, '0000000000000928': 928, '0000000000000929': 929, '0000000000000930': 930, '0000000000000931': 931, '0000000000000932': 932, '0000000000000933': 933, '0000000000000934': 934, '0000000000000935': 935, '0000000000000936': 936, '0000000000000937': 937, '0000000000000938': 938, '0000000000000939': 939, '0000000000000940': 940, '0000000000000941': 941, '0000000000000942': 942, '0000000000000943': 943, '0000000000000944': 944, '0000000000000945': 945, '0000000000000946': 946, '0000000000000947': 947, '0000000000000948': 948, '0000000000000949': 949, '0000000000000950': 950, '0000000000000951': 951, '0000000000000952': 952, '0000000000000953': 953, '0000000000000954': 954, '0000000000000955': 955, '0000000000000956': 956, '0000000000000957': 957, '0000000000000958': 958, '0000000000000959': 959, '0000000000000960': 960, '0000000000000961': 961, '0000000000000962': 962, '0000000000000963': 963, '0000000000000964': 964, '0000000000000965': 965, '0000000000000966': 966, '0000000000000967': 967, '0000000000000968': 968, '0000000000000969': 969, '0000000000000970': 970, '0000000000000971': 971, '0000000000000972': 972, '0000000000000973': 973, '0000000000000974': 974, '0000000000000975': 975, '0000000000000976': 976, '0000000000000977': 977, '0000000000000978': 978, '0000000000000979': 979, '0000000000000980': 980, '0000000000000981': 981, '0000000000000982': 982, '0000000000000983': 983, '0000000000000984': 984, '0000000000000985': 985, '0000000000000986': 986, '0000000000000987': 987, '0000000000000988': 988, '0000000000000989': 989, '0000000000000990': 990, '0000000000000991': 991, '0000000000000992': 992, '0000000000000993': 993, '0000000000000994': 994, '0000000000000995': 995, '0000000000000996': 996, '0000000000000997': 997, '0000000000000998': 998, '0000000000000999': 999}\n",
            "{'knife': 0}\n",
            "classes in directory doesn't match classes_num\n",
            "{'542': 0, '559': 1, '567': 2, '623': 3, '737': 4, '738': 5, '809': 6, '852': 7, '910': 8}\n",
            "classes in directory doesn't match classes_num\n",
            "50000 0\n",
            "2434 609\n",
            "72 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxB4w5RUD0kw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60ef1d16-ea62-4534-c9a3-0530cf268676"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Loss function\n",
        "def original_loss(y_true, y_pred):\n",
        "    n_dim = np.shape(y_pred)[0]  # number of features vecs\n",
        "    k_dim = np.shape(y_pred)[1]  # feature vec dim\n",
        "    lc = 1/(k_dim*n_dim)* n_dim**2 * K.sum((y_pred - K.mean(y_pred,axis=0))**2,axis=[1]) / ((n_dim-1)**2)\n",
        "    return lc\n",
        "\n",
        "#Learning\n",
        "def train(target_dataloader, reference_dataloader, epoch_num):\n",
        "\n",
        "    print(\"Model build...\")\n",
        "\n",
        "    network = network_constractor()\n",
        "\n",
        "\n",
        "    #Fixed weight\n",
        "    for layer in network.layers:\n",
        "        if layer.name == first_trained_layer_name:\n",
        "            break\n",
        "        else:\n",
        "            layer.trainable = False\n",
        "\n",
        "    model_t = Model(inputs=network.input,outputs=network.layers[-2].output)\n",
        "\n",
        "    #Apply a Fully Connected Layer to R\n",
        "    # prediction = Dense(classes, activation='softmax')(model_t.output)\n",
        "    model_r = Model(inputs=model_t.input,outputs=network.layers[-1].output)\n",
        "\n",
        "    \n",
        "    #Compile\n",
        "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    optimizer = SGD(lr=5e-5, decay=0.00005)\n",
        "    model_r.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[train_accuracy])\n",
        "    model_t.compile(optimizer=optimizer, loss=original_loss)\n",
        "\n",
        "\n",
        "    model_t.summary()\n",
        "    model_r.summary()\n",
        "\n",
        "    print(\"x_target is\", len(target_dataloader),'samples')\n",
        "    print(\"x_ref is\",len(reference_dataloader),'samples')\n",
        "\n",
        "    outputs = {\"d loss\": [], \"c loss\": [], \"accuracy\": []}\n",
        "\n",
        "    loss, loss_c, epoch_accuracy = [], [], []\n",
        "\n",
        "    epochs_writer = tf.summary.create_file_writer(logdir=epochs_log_dir)\n",
        "    batchs_writer = tf.summary.create_file_writer(logdir=batchs_log_dir)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"training...\")\n",
        "\n",
        "    #Learning\n",
        "    for epochnumber in range(epoch_num):\n",
        "        lc, ld, accuracy = [], [], []\n",
        "        for i in range(int(len(target_dataloader) / batchsize)):\n",
        "            #Load data for batch size\n",
        "            batch_is_ready = False\n",
        "            while not batch_is_ready:\n",
        "              try:\n",
        "                batch_target, _ = target_dataloader.next()\n",
        "                batch_is_ready = True\n",
        "              except:\n",
        "                if not target_dataloader.has_next():\n",
        "                    target_dataloader.on_epoch_end()\n",
        "                continue\n",
        "\n",
        "            batch_ref, batch_y = reference_dataloader.next()\n",
        "\n",
        "\n",
        "            #target data\n",
        "            #Get loss while learning\n",
        "            lc.append(model_t.train_on_batch(batch_target, np.zeros((batchsize, 4096))))\n",
        "\n",
        "            #reference data\n",
        "            #Get loss while learning\n",
        "            ref_output = model_r.train_on_batch(batch_ref, batch_y)\n",
        "            ld.append(ref_output[0])\n",
        "            accuracy.append(ref_output[1])\n",
        "\n",
        "            count = (epochnumber * int(len(target_dataloader) / batchsize)) + i\n",
        "\n",
        "            if count % 50 == 0:\n",
        "              with batchs_writer.as_default():\n",
        "                tf.summary.scalar(\"d loss\", np.mean(ld), step=count)\n",
        "                tf.summary.scalar(\"c loss\", np.mean(lc), step=count)\n",
        "                tf.summary.scalar(\"accuracy\", np.mean(accuracy), step=count)\n",
        "              print(\"batch: {}: d loss {}, c loss {}, accuracy {}\".format(count, np.mean(ld), np.mean(lc), np.mean(accuracy)))\n",
        "\n",
        "\n",
        "\n",
        "        target_dataloader.on_epoch_end()\n",
        "        reference_dataloader.on_epoch_end()\n",
        "\n",
        "        outputs[\"d loss\"].append(np.mean(ld))\n",
        "        outputs[\"c loss\"].append(np.mean(lc))\n",
        "        outputs[\"accuracy\"].append(np.mean(accuracy))\n",
        "        with epochs_writer.as_default():\n",
        "          for key in outputs:\n",
        "            tf.summary.scalar(key, outputs[key][-1], step=epochnumber+1)\n",
        "        print(\"epoch: {}: d loss {}, c loss {}, accuracy {}\".format(epochnumber+1, outputs[\"d loss\"][-1], outputs[\"c loss\"][-1], outputs[\"accuracy\"][-1] ))\n",
        "\n",
        "        checkpoint_path = \"weights_after_{}_epochs\".format(epochnumber+1)\n",
        "        network.save_weights(os.path.join(ckpt_path, checkpoint_path))\n",
        "\n",
        "\n",
        "\n",
        "    #Result graph\n",
        "    plt.plot(loss,label=\"Descriptive loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(loss_c,label=\"Compact loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend()\n",
        "    plt.show() \n",
        "\n",
        "    plt.plot(epoch_accuracy,label=\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend()\n",
        "    plt.show()   \n",
        "\n",
        "    return model_t\n",
        "\n",
        "\n",
        "\n",
        "model = train(train_s_loader, ref_loader, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model build...\n",
            "Model: \"functional_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 126,625,280\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n",
            "Model: \"functional_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 130,722,280\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n",
            "x_target is 2434 samples\n",
            "x_ref is 50000 samples\n",
            "training...\n",
            "batch: 0: d loss 2.9098055362701416, c loss 0.9585555195808411, accuracy 0.5\n",
            "batch: 50: d loss 2.8620226751355564, c loss 1.0280064502183128, accuracy 0.4411764705882353\n",
            "batch: 100: d loss 2.527101621585022, c loss 0.8816310896141695, accuracy 0.46534653465346537\n",
            "batch: 150: d loss 2.5318086109269067, c loss 0.7997033826957475, accuracy 0.45695364238410596\n",
            "batch: 200: d loss 2.482661048289787, c loss 0.7410010134995874, accuracy 0.4577114427860697\n",
            "batch: 250: d loss 2.4333759164372717, c loss 0.6978976990122244, accuracy 0.4721115537848606\n",
            "batch: 300: d loss 2.492135595602209, c loss 0.6665591082699671, accuracy 0.4568106312292359\n",
            "batch: 350: d loss 2.478804121014829, c loss 0.634188748662628, accuracy 0.45584045584045585\n",
            "batch: 400: d loss 2.4831776772736065, c loss 0.6036475105327264, accuracy 0.4625935162094763\n",
            "batch: 450: d loss 2.4754078514866755, c loss 0.5808332539318406, accuracy 0.4611973392461197\n",
            "batch: 500: d loss 2.4418925992208544, c loss 0.5597790621235937, accuracy 0.4720558882235529\n",
            "batch: 550: d loss 2.4600584078060392, c loss 0.5392648281289532, accuracy 0.4646098003629764\n",
            "batch: 600: d loss 2.442902937505515, c loss 0.522004374784856, accuracy 0.47088186356073214\n",
            "batch: 650: d loss 2.4326723811710926, c loss 0.5110924444325875, accuracy 0.4715821812596006\n",
            "batch: 700: d loss 2.4295692649552367, c loss 0.4988673675736245, accuracy 0.4664764621968616\n",
            "batch: 750: d loss 2.422520977152591, c loss 0.48684553553991405, accuracy 0.46804260985352863\n",
            "batch: 800: d loss 2.4241639037471794, c loss 0.47832344390554227, accuracy 0.4675405742821473\n",
            "batch: 850: d loss 2.4084769994664508, c loss 0.4689240535999157, accuracy 0.4688601645123384\n",
            "batch: 900: d loss 2.4148553125118397, c loss 0.4600869996036991, accuracy 0.46503884572697\n",
            "batch: 950: d loss 2.402359433112995, c loss 0.45352591634517464, accuracy 0.46898002103049424\n",
            "batch: 1000: d loss 2.3809992331170853, c loss 0.4466635253462758, accuracy 0.471028971028971\n",
            "batch: 1050: d loss 2.3727080045185085, c loss 0.4389334795207891, accuracy 0.4743101807802093\n",
            "batch: 1100: d loss 2.3839861074173663, c loss 0.43163440263509534, accuracy 0.47275204359673023\n",
            "batch: 1150: d loss 2.3894932838013196, c loss 0.42447528336925366, accuracy 0.47132927888792353\n",
            "batch: 1200: d loss 2.3978218692957776, c loss 0.41879426232608136, accuracy 0.46960865945045793\n",
            "epoch: 1: d loss 2.4014868723910037, c loss 0.41621900299042097, accuracy 0.4683648315529992\n",
            "batch: 1250: d loss 2.4421202102128197, c loss 0.2536790822358692, accuracy 0.4411764705882353\n",
            "batch: 1300: d loss 2.3487070411266315, c loss 0.26233197433785316, accuracy 0.47619047619047616\n",
            "batch: 1350: d loss 2.2325658232613064, c loss 0.2565471901377635, accuracy 0.48880597014925375\n",
            "batch: 1400: d loss 2.250296812939822, c loss 0.2591534209154222, accuracy 0.5135869565217391\n",
            "batch: 1450: d loss 2.1996232948517704, c loss 0.25739968915143585, accuracy 0.5299145299145299\n",
            "batch: 1500: d loss 2.20640207667758, c loss 0.25614991445671503, accuracy 0.5246478873239436\n",
            "batch: 1550: d loss 2.2205241907999325, c loss 0.25529031326135476, accuracy 0.5209580838323353\n",
            "batch: 1600: d loss 2.2010699971945846, c loss 0.25624442626334104, accuracy 0.5260416666666666\n",
            "batch: 1650: d loss 2.2005865308029624, c loss 0.2536081168113903, accuracy 0.5311059907834101\n",
            "batch: 1700: d loss 2.151803486209607, c loss 0.2546495879684721, accuracy 0.5320247933884298\n",
            "batch: 1750: d loss 2.1455007616894, c loss 0.2538103955808315, accuracy 0.5271535580524345\n",
            "batch: 1800: d loss 2.173954717539632, c loss 0.25120669878279306, accuracy 0.523972602739726\n",
            "batch: 1850: d loss 2.159075210122678, c loss 0.25091899325630457, accuracy 0.5252365930599369\n",
            "batch: 1900: d loss 2.1731470570351847, c loss 0.2485533216314619, accuracy 0.5219298245614035\n",
            "batch: 1950: d loss 2.1946301069457976, c loss 0.24517561676493296, accuracy 0.5190735694822888\n",
            "batch: 2000: d loss 2.196741980000854, c loss 0.24487867240546918, accuracy 0.514030612244898\n",
            "batch: 2050: d loss 2.18063873138736, c loss 0.24370721487297142, accuracy 0.5167865707434053\n",
            "batch: 2100: d loss 2.1839401011487705, c loss 0.242605489421619, accuracy 0.5130090497737556\n",
            "batch: 2150: d loss 2.1897142271562857, c loss 0.24246377535403668, accuracy 0.5101713062098501\n",
            "batch: 2200: d loss 2.2100690429308782, c loss 0.24068731379248504, accuracy 0.505589430894309\n",
            "batch: 2250: d loss 2.2316121645392246, c loss 0.2388882915015608, accuracy 0.5024177949709865\n",
            "batch: 2300: d loss 2.230896219880405, c loss 0.2384519719508411, accuracy 0.503690036900369\n",
            "batch: 2350: d loss 2.222456007395445, c loss 0.23824339817283013, accuracy 0.5066137566137566\n",
            "batch: 2400: d loss 2.219233193668687, c loss 0.23725999236685802, accuracy 0.5054898648648649\n",
            "epoch: 2: d loss 2.2287540166001136, c loss 0.23644905441250091, accuracy 0.5036976170912079\n",
            "batch: 2450: d loss 2.7057461230193867, c loss 0.18570496317218332, accuracy 0.5\n",
            "batch: 2500: d loss 2.2367479423644827, c loss 0.2120447504097846, accuracy 0.5298507462686567\n",
            "batch: 2550: d loss 2.2340485114508715, c loss 0.1985029600815386, accuracy 0.5341880341880342\n",
            "batch: 2600: d loss 2.1462459826791758, c loss 0.2061236442964591, accuracy 0.5329341317365269\n",
            "batch: 2650: d loss 2.209705600560072, c loss 0.20440188906915177, accuracy 0.511520737327189\n",
            "batch: 2700: d loss 2.178233529313743, c loss 0.2086105055074567, accuracy 0.5205992509363296\n",
            "batch: 2750: d loss 2.1138709165957943, c loss 0.20790251645286, accuracy 0.5236593059936908\n",
            "batch: 2800: d loss 2.1503383133227985, c loss 0.20132136085052907, accuracy 0.5122615803814714\n",
            "batch: 2850: d loss 2.11387011084183, c loss 0.19943597759131335, accuracy 0.5179856115107914\n",
            "batch: 2900: d loss 2.132017371945713, c loss 0.20301619221554068, accuracy 0.5192719486081371\n",
            "batch: 2950: d loss 2.1723775365570024, c loss 0.20217986955402897, accuracy 0.5087040618955513\n",
            "batch: 3000: d loss 2.16640162052475, c loss 0.20403279391703782, accuracy 0.5097001763668431\n",
            "batch: 3050: d loss 2.17017513777792, c loss 0.20314185317708375, accuracy 0.5129659643435981\n",
            "batch: 3100: d loss 2.1626834560540327, c loss 0.20438962859817889, accuracy 0.5134932533733133\n",
            "batch: 3150: d loss 2.1544304965401646, c loss 0.20272052756184647, accuracy 0.5118549511854951\n",
            "batch: 3200: d loss 2.1567383313204274, c loss 0.20124550939645652, accuracy 0.5130378096479792\n",
            "batch: 3250: d loss 2.1630665380712895, c loss 0.20165865308624514, accuracy 0.5134638922888617\n",
            "batch: 3300: d loss 2.1575856713949775, c loss 0.20114474296097182, accuracy 0.5178777393310265\n",
            "batch: 3350: d loss 2.156544736251357, c loss 0.19985041065566087, accuracy 0.5185387131952017\n",
            "batch: 3400: d loss 2.1608656655694625, c loss 0.19825899572282135, accuracy 0.515511892450879\n",
            "batch: 3450: d loss 2.153625403151773, c loss 0.1980776843709199, accuracy 0.5162241887905604\n",
            "batch: 3500: d loss 2.1357417216754913, c loss 0.19954089858615465, accuracy 0.520618556701031\n",
            "batch: 3550: d loss 2.122489449159545, c loss 0.1992040882162223, accuracy 0.5237242614145031\n",
            "batch: 3600: d loss 2.1213095308307017, c loss 0.19889628335086576, accuracy 0.5231362467866324\n",
            "batch: 3650: d loss 2.122290330215441, c loss 0.1987223935343368, accuracy 0.5250616269515201\n",
            "epoch: 3: d loss 2.122290330215441, c loss 0.1987223935343368, accuracy 0.5250616269515201\n",
            "batch: 3700: d loss 2.0608337973617017, c loss 0.16470814280211926, accuracy 0.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMcZZ3Z7N3-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X_train_s, relevant_paths_train_s = train_s_loader.get_all_data(size=40)\n",
        "X_test_s, relevant_paths_test_s = test_s_loader.get_all_data(size=70)\n",
        "X_test_b, relevant_paths_test_b = test_b_loader.get_all_data(size=70)\n",
        "\n",
        "train = model.predict(X_train_s)\n",
        "test_s = model.predict(X_test_s)\n",
        "test_b = model.predict(X_test_b)\n",
        "\n",
        "train = train.reshape((len(X_train_s),-1))\n",
        "test_s = test_s.reshape((len(X_test_s),-1))\n",
        "test_b = test_b.reshape((len(X_test_b),-1))\n",
        "\n",
        "#Convert to 0-1\n",
        "ms = MinMaxScaler()\n",
        "train = ms.fit_transform(train)\n",
        "test_s = ms.transform(test_s)\n",
        "test_b = ms.transform(test_b)\n",
        "\n",
        "# fit the model\n",
        "clf = LocalOutlierFactor(n_neighbors=5)\n",
        "y_pred = clf.fit(train)\n",
        "\n",
        "#Abnormal score\n",
        "Z1 = -clf._decision_function(test_s)\n",
        "Z2 = -clf._decision_function(test_b)\n",
        "\n",
        "#Drawing of ROC curve\n",
        "y_true = np.zeros(len(test_s)+len(test_b))\n",
        "y_true[len(test_s):] = 1 #0:Normal, 1：Abnormal\n",
        "\n",
        "#Calculate FPR, TPR(, Threshould)\n",
        "fpr, tpr, _ = metrics.roc_curve(y_true, np.hstack((Z1, Z2)))\n",
        "\n",
        "#AUC\n",
        "auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "#Plot the ROC curve\n",
        "plt.plot(fpr, tpr, label='DeepOneClassification(AUC = %.2f)'%auc)\n",
        "plt.legend()\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4jO1LYCDNXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRED_S_RESULTS_PATH = os.path.join(output_path, \"pred_s_results.txt\")\n",
        "PRED_B_RESULTS_PATH = os.path.join(output_path, \"pred_b_results.txt\")\n",
        "\n",
        "\n",
        "with open(PRED_S_RESULTS_PATH, \"w\") as rf:\n",
        "  test_s_scores = list(zip(relevant_paths_test_s, Z1))\n",
        "  for item in test_s_scores:\n",
        "    f.write(\"%s\\n\" % item)\n",
        "\n",
        "with open(PRED_B_RESULTS_PATH, \"w\") as rf:\n",
        "  test_b_scores = list(zip(relevant_paths_test_b, Z2))\n",
        "  for item in test_b_scores:\n",
        "    f.write(\"%s\\n\" % item)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-bj0yDpkLga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "new_network = network_constractor()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# new_mobile = tf.keras.applications.MobileNetV2(include_top=True, input_shape=(size, size, 3), alpha=alpha, weights='imagenet')\n",
        "\n",
        "untrained_test_s = new_network.predict(X_test_s)\n",
        "untrained_test_b = new_network.predict(X_test_b)\n",
        "\n",
        "untrained_test_s_embedded = TSNE(n_components=2).fit_transform(test_s)\n",
        "untrained_test_b_embedded = TSNE(n_components=2).fit_transform(test_b)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(untrained_test_s_embedded[:, 0],untrained_test_s_embedded[:,1], label=\"Sneakers\")\n",
        "plt.scatter(untrained_test_b_embedded[:, 0],untrained_test_b_embedded[:,1], label=\"Boots\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "test_s_embedded = TSNE(n_components=2).fit_transform(test_s)\n",
        "test_b_embedded = TSNE(n_components=2).fit_transform(test_b)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(test_s_embedded[:, 0],test_s_embedded[:,1], label=\"Sneakers\")\n",
        "plt.scatter(test_b_embedded[:, 0],test_b_embedded[:,1], label=\"Boots\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}