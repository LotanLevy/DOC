{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jupyter_code.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOz9v6QDMjUfOnzNCq07iEo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LotanLevy/DOC/blob/master/jupyter_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUwpGRq-7bvr"
      },
      "source": [
        "### **Enviroment settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EpcrZ0KE25J",
        "outputId": "912f0135-2d73-40ec-bfed-fc3701116ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "pip install tensorflow-gpu==2.3.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/11/763f55d3d15efd778ef24453f126e6c33635680e5a2bb346da3fab5997cb/tensorflow_gpu-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 49kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.32.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (0.10.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu==2.3.0) (50.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.0) (3.2.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecouf20wMZcO",
        "outputId": "1932f6b1-5e2f-4488-bbb8-c2db64a65d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL57WBVF74NY"
      },
      "source": [
        "### **Usefull Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygOL2asKHd8K"
      },
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications import mobilenet_v2, vgg16\n",
        "\n",
        "\"\"\"\n",
        "resize function for mnist data\n",
        "\"\"\"\n",
        "def resize(x, size=96):\n",
        "    x_out = []\n",
        "    for i in range(len(x)):\n",
        "      img = x[i]\n",
        "      if img.shape[-1] == 1:\n",
        "        img = cv2.cvtColor(x[i], cv2.COLOR_GRAY2RGB)\n",
        "      img = cv2.resize(img,dsize=(size,size))\n",
        "      x_out.append(img)\n",
        "    return np.array(x_out)\n",
        "\n",
        "\"\"\"\n",
        "Mobile net network faster then vgg but acheive worse results than vgg.\n",
        "Now only work for fmnist\n",
        "\"\"\"\n",
        "def mobilenet_v2_preprocessing(input_data):\n",
        "  return input_data.astype('float32')/ 255\n",
        "\n",
        "\"\"\"\n",
        "Should work for both mnist and imagenet test\n",
        "\"\"\"\n",
        "def vgg_preprocessing(input_data):\n",
        "  # return vgg16.preprocess_input(resize(np.copy(input_data).astype('float32'), size=size))\n",
        "    return vgg16.preprocess_input(np.copy(input_data.astype('float32')))\n",
        "\n",
        "def read_paths_file(path_to_paths_file):\n",
        "  with open(path_to_paths_file, 'r') as f:\n",
        "    return [line[:-1] for line in f]\n",
        "\n",
        "def read_labels_file(path_to_labels_file):\n",
        "  with open(path_to_labels_file, 'r') as f:\n",
        "    return [int(line[:-1]) for line in f]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIEoU55O9kbG"
      },
      "source": [
        "### **Dataloaders classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHo-SyQ4cVZC"
      },
      "source": [
        "import os\n",
        "import numpy as np \n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Data loader for fmnist data (gets the full data and their labels in the constructor)\n",
        "\"\"\"\n",
        "class DataIter:\n",
        "  def __init__(self, data, labels, batch_size, classes_num, shuffle=False, preprocess_func=lambda x:x):\n",
        "    if labels is not None:\n",
        "        assert(len(data) == len(labels))\n",
        "        self.labels = np.array(labels)\n",
        "    else:\n",
        "      self.labels = None\n",
        "\n",
        "    self.preprocess_func = preprocess_func\n",
        "\n",
        "    self.data = data\n",
        "    self.classes_num = classes_num\n",
        "    self.batch_size = batch_size\n",
        "    self.indices = np.arange(len(self.data)).astype(np.int)\n",
        "    self.shuffle = shuffle\n",
        "    self.on_epoch_end()\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(self.indices)\n",
        "    self.cur_idx = 0\n",
        "\n",
        "  def next(self):\n",
        "    relevant_indices = self.indices[self.cur_idx: self.cur_idx + self.batch_size]\n",
        "    self.cur_idx += self.batch_size\n",
        "    images = self.data[relevant_indices]\n",
        "    if self.labels is not None:\n",
        "      labels = self.labels[relevant_indices]\n",
        "      labels = tf.keras.utils.to_categorical(labels, num_classes=self.classes_num)\n",
        "    else:\n",
        "      labels = None\n",
        "    return resize(self.preprocess_func(images), size), labels\n",
        "\n",
        "  def get_all_data(self, size=None):\n",
        "    if size is None:\n",
        "      size = len(self.data)\n",
        "    return resize(self.preprocess_func(self.data[:size]), size), None\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Data iterator that loads the data from a path only when a batch called (next and get_all_data)\n",
        "\"\"\"\n",
        "class DirIter:\n",
        "    def __init__(self, paths, labels, batch_size, input_size, classes_num, shuffle=False, preprocess_func=lambda x:x):\n",
        "      if labels is not None:\n",
        "        assert(len(paths) == len(labels))\n",
        "        self.labels = np.array(labels)\n",
        "      else:\n",
        "        self.labels = None\n",
        "      self.preprocess_func = preprocess_func\n",
        "\n",
        "      self.paths = paths\n",
        "      self.classes_num = classes_num\n",
        "      self.batch_size = batch_size\n",
        "      self.indices = np.arange(len(self.paths)).astype(np.int)\n",
        "      self.input_size = input_size\n",
        "      self.shuffle = shuffle\n",
        "      self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.paths)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "      if self.shuffle and len(self.indices) > 0:\n",
        "        np.random.shuffle(self.indices)\n",
        "      self.cur_idx = 0\n",
        "\n",
        "    def load_img(self, image_path):\n",
        "        image = Image.open(image_path, 'r')\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "        image = image.resize(self.input_size, Image.NEAREST)\n",
        "        image = np.array(image).astype(np.float32)\n",
        "        return np.expand_dims(image, axis=0)\n",
        "\n",
        "    def next(self):\n",
        "        relevant_indices = self.indices[self.cur_idx: self.cur_idx + self.batch_size]\n",
        "        self.cur_idx += self.batch_size\n",
        "        images = []\n",
        "        images = np.concatenate([self.load_img(self.paths[i]) for i in relevant_indices])\n",
        "        if self.labels is not None:\n",
        "          labels = self.labels[relevant_indices]\n",
        "          labels = tf.keras.utils.to_categorical(labels, num_classes=self.classes_num)\n",
        "        else:\n",
        "          labels = None\n",
        "        return self.preprocess_func(images), labels\n",
        "\n",
        "    def get_all_data(self, size=None):\n",
        "      if size is None:\n",
        "        size = len(self.paths)\n",
        "      relevant_paths = [self.paths[i] for i in self.indices[:size]]\n",
        "      images = np.concatenate([self.load_img(path) for path in relevant_paths])\n",
        "      return self.preprocess_func(images), relevant_paths\n",
        "\n",
        "    def has_next(self):\n",
        "      return self.cur_idx + self.batch_size < len(self.indices)\n",
        "\n",
        "    def set_cls2label_map(self, map):\n",
        "        self.cls2label = map\n",
        "\n",
        "    def write_data(self, output_path, loader_name):\n",
        "      with open(os.path.join(output_path, loader_name + \"_paths.txt\"), 'w') as f:\n",
        "        for p in self.paths:\n",
        "          f.write(p + \"\\n\")\n",
        "      with open(os.path.join(output_path, loader_name + \"_labels.txt\"), 'w') as f:\n",
        "        for l in self.labels:\n",
        "          f.write(str(l) + \"\\n\")\n",
        "\n",
        "def construct_with_files(path_file, label_file, batch_size, input_size, classes_num, shuffle=False, preprocess_func=lambda x:x):\n",
        "  paths = read_paths_file(path_file)\n",
        "  if label_file is not None:\n",
        "    labels = read_labels_file(label_file)\n",
        "  else:\n",
        "    labels = None\n",
        "  return DirIter(paths, labels, batch_size, input_size, classes_num, shuffle, preprocess_func)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Data loader that loads data from a dir of sub dirs, each sub dir contains data of one class.\n",
        "Works similarly to directory iterator, but it loads the data from a path only when a batch called (next and get_all_data)\n",
        "\"\"\"\n",
        "def get_iterators_by_root_dir(root_dir, batch_size, input_size, split_val, classes_num, shuffle=False, preprocess_func=lambda x:x):\n",
        "    dirs = os.listdir(root_dir)\n",
        "    length = len(max(dirs, key=len))\n",
        "\n",
        "    for dir in dirs: # Handle the sort problem pads the clas num with '0'\n",
        "        if len(dir) < length:\n",
        "          zeros = \"0\" * (length - len(dir))\n",
        "          new_name = zeros + dir\n",
        "\n",
        "          os.rename(os.path.join(root_dir, dir), os.path.join(root_dir, new_name))\n",
        "          print(\"old {}, new {}\".format(dir, new_name))\n",
        "\n",
        "    paths = []\n",
        "    labels = []\n",
        "    cls2label = dict()\n",
        "    label_idx = 0\n",
        "    for sub_dir in sorted(os.listdir(root_dir)):\n",
        "\n",
        "        full_path = os.path.join(root_dir, sub_dir)\n",
        "        if not os.path.isdir(full_path):\n",
        "            continue\n",
        "        cls2label[sub_dir] = label_idx\n",
        "        for file in os.listdir(full_path):\n",
        "            paths.append(os.path.join(full_path, file))\n",
        "            labels.append(label_idx)\n",
        "        label_idx += 1\n",
        "\n",
        "    print(cls2label)\n",
        "\n",
        "\n",
        "    assert len(paths) == len(labels)\n",
        "    if len(cls2label) != classes_num:\n",
        "        print(\"classes in directory doesn't match classes_num\")\n",
        "\n",
        "    if split_val > 0:\n",
        "      X_train, X_test, y_train, y_test = train_test_split(paths, labels, test_size=split_val, shuffle=shuffle)\n",
        "    else:\n",
        "      X_train, X_test, y_train, y_test = paths, [], labels, []\n",
        "\n",
        "    train_iter = DirIter(X_train, y_train, batch_size, input_size, classes_num, shuffle=True, preprocess_func=preprocess_func)\n",
        "    val_iter = DirIter(X_test, y_test, batch_size, input_size, classes_num, shuffle=True, preprocess_func=preprocess_func)\n",
        "\n",
        "    train_iter.set_cls2label_map(cls2label)\n",
        "    val_iter.set_cls2label_map(cls2label)\n",
        "    return train_iter, val_iter\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkCrf-M--2H4"
      },
      "source": [
        "###**Dataset dataloaders constructors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbTH9ooTDivN"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "\n",
        "def get_fmnist_data_loaders():\n",
        "  # dataset\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "  #Splitting the into subsets data\n",
        "  x_train_s, x_test_s, x_test_b = [], [], []\n",
        "  x_ref, y_ref = [], []\n",
        "\n",
        "  x_train_shape = x_train.shape\n",
        "  #train data\n",
        "  for i in range(len(x_train)):\n",
        "      if y_train[i] == 7: #Sneakers is 7\n",
        "          temp = x_train[i]\n",
        "          x_train_s.append(temp.reshape((x_train_shape[1:])))\n",
        "      else:\n",
        "          temp = x_train[i]\n",
        "          x_ref.append(temp.reshape((x_train_shape[1:])))\n",
        "          y_ref.append(y_train[i])\n",
        "\n",
        "  x_ref, y_ref = np.array(x_ref), np.array(y_ref)\n",
        "\n",
        "  #test data\n",
        "  for i in range(len(x_test)):\n",
        "      if y_test[i] == 7: #Sneakers is 7\n",
        "          temp = x_test[i,:,:,:]\n",
        "          x_test_s.append(temp.reshape((x_train_shape[1:])))\n",
        "\n",
        "      if y_test[i] == 9: #Boots is 9\n",
        "          temp = x_test[i,:,:,:]\n",
        "          x_test_b.append(temp.reshape((x_train_shape[1:])))\n",
        "\n",
        "  #tdata loaders\n",
        "  train_s_loader = DataIter(np.array(x_train_s), None, batchsize, classes, shuffle=True, preprocess_func=preprocessing_func)\n",
        "  ref_loader = DataIter(np.array(x_ref), np.array(y_ref), batchsize, classes, shuffle=True, preprocess_func=preprocessing_func)\n",
        "\n",
        "  test_s_loader = DataIter(np.array(x_test_s), None, batchsize, classes, preprocess_func=preprocessing_func)\n",
        "  test_b_loader = DataIter(np.array(x_test_b), None, batchsize, classes, preprocess_func=preprocessing_func)\n",
        "  return  train_s_loader, ref_loader, test_s_loader, test_b_loader\n",
        "\n",
        "\n",
        "def get_imagenet_clatech_loaders(ref_path, tar_path, alien_path, batchsize, input_size, split_val, cls_num, shuffle=False, preprocess_func=lambda x:x):\n",
        "  ref_loader, i1 = get_iterators_by_root_dir(ref_path, batchsize, input_size, 0, cls_num, shuffle=shuffle, preprocess_func=preprocess_func)\n",
        "  train_s_loader, test_s_loader = get_iterators_by_root_dir(tar_path, batchsize, input_size, split_val, cls_num, shuffle=shuffle, preprocess_func=preprocess_func)\n",
        "  test_alien_loader, i2 = get_iterators_by_root_dir(alien_path, batchsize, input_size, 0, cls_num, shuffle=shuffle, preprocess_func=preprocess_func)\n",
        "  print(len(ref_loader), len(i1))\n",
        "  print(len(train_s_loader), len(test_s_loader))\n",
        "  print(len(test_alien_loader), len(i2))\n",
        "\n",
        "  return  train_s_loader, ref_loader, test_s_loader, test_alien_loader"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZgGJobg_KvE"
      },
      "source": [
        "## **Run Settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjiyMA8pu6gY",
        "outputId": "557ffc18-3463-4c0f-ca6d-18d8912be518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "\"\"\"\n",
        "Outputs directories\n",
        "\"\"\"\n",
        "# main output folder\n",
        "output_path = \"/content/drive/My Drive/Colab Notebooks/affordances/experiments/experiment_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "if not os.path.exists(output_path):\n",
        "  os.makedirs(output_path)\n",
        "#check points folder\n",
        "ckpt_path = os.path.join(output_path, \"ckpts\")\n",
        "if not os.path.exists(ckpt_path):\n",
        "  os.makedirs(ckpt_path)\n",
        "#Logs folder\n",
        "epochs_log_dir = os.path.join(os.path.join(output_path, \"epochs_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
        "batchs_log_dir = os.path.join(os.path.join(output_path, \"batchs_logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
        "\n",
        "\n",
        "# MobileNetV2\n",
        "# size = 96\n",
        "# preprocessing_func = mobilenet_v2_preprocessing\n",
        "# first_trained_layer_name = \"block_13_expand\"\n",
        "# alpha = 0.5 #for MobileNetV2\n",
        "# network_constractor = lambda : tf.keras.applications.MobileNetV2(include_top=True, input_shape=(size, size, 3), alpha=alpha, weights='imagenet')\n",
        "\n",
        "\n",
        "# VGG16\n",
        "size = 224\n",
        "preprocessing_func = vgg_preprocessing\n",
        "first_trained_layer_name =  \"block5_conv1\"\n",
        "network_constractor = lambda : tf.keras.applications.VGG16(include_top=True, input_shape=(size, size, 3), weights='imagenet')\n",
        "test_size = 200\n",
        "\n",
        "\n",
        "\n",
        "lambda_ = 0.1 #for compact loss\n",
        "\n",
        "# fmnist data\n",
        "# classes = 10\n",
        "# batchsize = 2\n",
        "# train_s_loader, ref_loader, test_s_loader, test_b_loader = get_fmnist_data_loaders()\n",
        "\n",
        "# # imagenet clatech data\n",
        "REFERENCE_PATH = \"/content/drive/My Drive/Colab Notebooks/affordances/datasets/imagenet_val_splitted\"\n",
        "TARGET_PATH = \"/content/drive/My Drive/Colab Notebooks/affordances/datasets/stab_data\"\n",
        "ALIEN_PATH = \"/content/drive/My Drive/Colab Notebooks/affordances/datasets/clean_alien_visulization\"\n",
        "classes = 1000\n",
        "batchsize = 2\n",
        "split_val = 0.2\n",
        "\n",
        "train_s_loader, ref_loader, test_s_loader, test_b_loader = get_imagenet_clatech_loaders(REFERENCE_PATH, TARGET_PATH, ALIEN_PATH, batchsize, (size,size), split_val, classes, shuffle=True, preprocess_func=preprocessing_func)\n",
        "\n",
        "path_file = (output_path, \"target_test\"+\"_paths.txt\")\n",
        "label_file = (output_path, \"target_test\"+\"_labels.txt\")\n",
        "\n",
        "\n",
        "train_s_loader.write_data(output_path, \"target_train\")\n",
        "test_s_loader.write_data(output_path, \"target_test\")\n",
        "test_b_loader.write_data(output_path, \"alien_test\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0000000000000000': 0, '0000000000000001': 1, '0000000000000002': 2, '0000000000000003': 3, '0000000000000004': 4, '0000000000000005': 5, '0000000000000006': 6, '0000000000000007': 7, '0000000000000008': 8, '0000000000000009': 9, '0000000000000010': 10, '0000000000000011': 11, '0000000000000012': 12, '0000000000000013': 13, '0000000000000014': 14, '0000000000000015': 15, '0000000000000016': 16, '0000000000000017': 17, '0000000000000018': 18, '0000000000000019': 19, '0000000000000020': 20, '0000000000000021': 21, '0000000000000022': 22, '0000000000000023': 23, '0000000000000024': 24, '0000000000000025': 25, '0000000000000026': 26, '0000000000000027': 27, '0000000000000028': 28, '0000000000000029': 29, '0000000000000030': 30, '0000000000000031': 31, '0000000000000032': 32, '0000000000000033': 33, '0000000000000034': 34, '0000000000000035': 35, '0000000000000036': 36, '0000000000000037': 37, '0000000000000038': 38, '0000000000000039': 39, '0000000000000040': 40, '0000000000000041': 41, '0000000000000042': 42, '0000000000000043': 43, '0000000000000044': 44, '0000000000000045': 45, '0000000000000046': 46, '0000000000000047': 47, '0000000000000048': 48, '0000000000000049': 49, '0000000000000050': 50, '0000000000000051': 51, '0000000000000052': 52, '0000000000000053': 53, '0000000000000054': 54, '0000000000000055': 55, '0000000000000056': 56, '0000000000000057': 57, '0000000000000058': 58, '0000000000000059': 59, '0000000000000060': 60, '0000000000000061': 61, '0000000000000062': 62, '0000000000000063': 63, '0000000000000064': 64, '0000000000000065': 65, '0000000000000066': 66, '0000000000000067': 67, '0000000000000068': 68, '0000000000000069': 69, '0000000000000070': 70, '0000000000000071': 71, '0000000000000072': 72, '0000000000000073': 73, '0000000000000074': 74, '0000000000000075': 75, '0000000000000076': 76, '0000000000000077': 77, '0000000000000078': 78, '0000000000000079': 79, '0000000000000080': 80, '0000000000000081': 81, '0000000000000082': 82, '0000000000000083': 83, '0000000000000084': 84, '0000000000000085': 85, '0000000000000086': 86, '0000000000000087': 87, '0000000000000088': 88, '0000000000000089': 89, '0000000000000090': 90, '0000000000000091': 91, '0000000000000092': 92, '0000000000000093': 93, '0000000000000094': 94, '0000000000000095': 95, '0000000000000096': 96, '0000000000000097': 97, '0000000000000098': 98, '0000000000000099': 99, '0000000000000100': 100, '0000000000000101': 101, '0000000000000102': 102, '0000000000000103': 103, '0000000000000104': 104, '0000000000000105': 105, '0000000000000106': 106, '0000000000000107': 107, '0000000000000108': 108, '0000000000000109': 109, '0000000000000110': 110, '0000000000000111': 111, '0000000000000112': 112, '0000000000000113': 113, '0000000000000114': 114, '0000000000000115': 115, '0000000000000116': 116, '0000000000000117': 117, '0000000000000118': 118, '0000000000000119': 119, '0000000000000120': 120, '0000000000000121': 121, '0000000000000122': 122, '0000000000000123': 123, '0000000000000124': 124, '0000000000000125': 125, '0000000000000126': 126, '0000000000000127': 127, '0000000000000128': 128, '0000000000000129': 129, '0000000000000130': 130, '0000000000000131': 131, '0000000000000132': 132, '0000000000000133': 133, '0000000000000134': 134, '0000000000000135': 135, '0000000000000136': 136, '0000000000000137': 137, '0000000000000138': 138, '0000000000000139': 139, '0000000000000140': 140, '0000000000000141': 141, '0000000000000142': 142, '0000000000000143': 143, '0000000000000144': 144, '0000000000000145': 145, '0000000000000146': 146, '0000000000000147': 147, '0000000000000148': 148, '0000000000000149': 149, '0000000000000150': 150, '0000000000000151': 151, '0000000000000152': 152, '0000000000000153': 153, '0000000000000154': 154, '0000000000000155': 155, '0000000000000156': 156, '0000000000000157': 157, '0000000000000158': 158, '0000000000000159': 159, '0000000000000160': 160, '0000000000000161': 161, '0000000000000162': 162, '0000000000000163': 163, '0000000000000164': 164, '0000000000000165': 165, '0000000000000166': 166, '0000000000000167': 167, '0000000000000168': 168, '0000000000000169': 169, '0000000000000170': 170, '0000000000000171': 171, '0000000000000172': 172, '0000000000000173': 173, '0000000000000174': 174, '0000000000000175': 175, '0000000000000176': 176, '0000000000000177': 177, '0000000000000178': 178, '0000000000000179': 179, '0000000000000180': 180, '0000000000000181': 181, '0000000000000182': 182, '0000000000000183': 183, '0000000000000184': 184, '0000000000000185': 185, '0000000000000186': 186, '0000000000000187': 187, '0000000000000188': 188, '0000000000000189': 189, '0000000000000190': 190, '0000000000000191': 191, '0000000000000192': 192, '0000000000000193': 193, '0000000000000194': 194, '0000000000000195': 195, '0000000000000196': 196, '0000000000000197': 197, '0000000000000198': 198, '0000000000000199': 199, '0000000000000200': 200, '0000000000000201': 201, '0000000000000202': 202, '0000000000000203': 203, '0000000000000204': 204, '0000000000000205': 205, '0000000000000206': 206, '0000000000000207': 207, '0000000000000208': 208, '0000000000000209': 209, '0000000000000210': 210, '0000000000000211': 211, '0000000000000212': 212, '0000000000000213': 213, '0000000000000214': 214, '0000000000000215': 215, '0000000000000216': 216, '0000000000000217': 217, '0000000000000218': 218, '0000000000000219': 219, '0000000000000220': 220, '0000000000000221': 221, '0000000000000222': 222, '0000000000000223': 223, '0000000000000224': 224, '0000000000000225': 225, '0000000000000226': 226, '0000000000000227': 227, '0000000000000228': 228, '0000000000000229': 229, '0000000000000230': 230, '0000000000000231': 231, '0000000000000232': 232, '0000000000000233': 233, '0000000000000234': 234, '0000000000000235': 235, '0000000000000236': 236, '0000000000000237': 237, '0000000000000238': 238, '0000000000000239': 239, '0000000000000240': 240, '0000000000000241': 241, '0000000000000242': 242, '0000000000000243': 243, '0000000000000244': 244, '0000000000000245': 245, '0000000000000246': 246, '0000000000000247': 247, '0000000000000248': 248, '0000000000000249': 249, '0000000000000250': 250, '0000000000000251': 251, '0000000000000252': 252, '0000000000000253': 253, '0000000000000254': 254, '0000000000000255': 255, '0000000000000256': 256, '0000000000000257': 257, '0000000000000258': 258, '0000000000000259': 259, '0000000000000260': 260, '0000000000000261': 261, '0000000000000262': 262, '0000000000000263': 263, '0000000000000264': 264, '0000000000000265': 265, '0000000000000266': 266, '0000000000000267': 267, '0000000000000268': 268, '0000000000000269': 269, '0000000000000270': 270, '0000000000000271': 271, '0000000000000272': 272, '0000000000000273': 273, '0000000000000274': 274, '0000000000000275': 275, '0000000000000276': 276, '0000000000000277': 277, '0000000000000278': 278, '0000000000000279': 279, '0000000000000280': 280, '0000000000000281': 281, '0000000000000282': 282, '0000000000000283': 283, '0000000000000284': 284, '0000000000000285': 285, '0000000000000286': 286, '0000000000000287': 287, '0000000000000288': 288, '0000000000000289': 289, '0000000000000290': 290, '0000000000000291': 291, '0000000000000292': 292, '0000000000000293': 293, '0000000000000294': 294, '0000000000000295': 295, '0000000000000296': 296, '0000000000000297': 297, '0000000000000298': 298, '0000000000000299': 299, '0000000000000300': 300, '0000000000000301': 301, '0000000000000302': 302, '0000000000000303': 303, '0000000000000304': 304, '0000000000000305': 305, '0000000000000306': 306, '0000000000000307': 307, '0000000000000308': 308, '0000000000000309': 309, '0000000000000310': 310, '0000000000000311': 311, '0000000000000312': 312, '0000000000000313': 313, '0000000000000314': 314, '0000000000000315': 315, '0000000000000316': 316, '0000000000000317': 317, '0000000000000318': 318, '0000000000000319': 319, '0000000000000320': 320, '0000000000000321': 321, '0000000000000322': 322, '0000000000000323': 323, '0000000000000324': 324, '0000000000000325': 325, '0000000000000326': 326, '0000000000000327': 327, '0000000000000328': 328, '0000000000000329': 329, '0000000000000330': 330, '0000000000000331': 331, '0000000000000332': 332, '0000000000000333': 333, '0000000000000334': 334, '0000000000000335': 335, '0000000000000336': 336, '0000000000000337': 337, '0000000000000338': 338, '0000000000000339': 339, '0000000000000340': 340, '0000000000000341': 341, '0000000000000342': 342, '0000000000000343': 343, '0000000000000344': 344, '0000000000000345': 345, '0000000000000346': 346, '0000000000000347': 347, '0000000000000348': 348, '0000000000000349': 349, '0000000000000350': 350, '0000000000000351': 351, '0000000000000352': 352, '0000000000000353': 353, '0000000000000354': 354, '0000000000000355': 355, '0000000000000356': 356, '0000000000000357': 357, '0000000000000358': 358, '0000000000000359': 359, '0000000000000360': 360, '0000000000000361': 361, '0000000000000362': 362, '0000000000000363': 363, '0000000000000364': 364, '0000000000000365': 365, '0000000000000366': 366, '0000000000000367': 367, '0000000000000368': 368, '0000000000000369': 369, '0000000000000370': 370, '0000000000000371': 371, '0000000000000372': 372, '0000000000000373': 373, '0000000000000374': 374, '0000000000000375': 375, '0000000000000376': 376, '0000000000000377': 377, '0000000000000378': 378, '0000000000000379': 379, '0000000000000380': 380, '0000000000000381': 381, '0000000000000382': 382, '0000000000000383': 383, '0000000000000384': 384, '0000000000000385': 385, '0000000000000386': 386, '0000000000000387': 387, '0000000000000388': 388, '0000000000000389': 389, '0000000000000390': 390, '0000000000000391': 391, '0000000000000392': 392, '0000000000000393': 393, '0000000000000394': 394, '0000000000000395': 395, '0000000000000396': 396, '0000000000000397': 397, '0000000000000398': 398, '0000000000000399': 399, '0000000000000400': 400, '0000000000000401': 401, '0000000000000402': 402, '0000000000000403': 403, '0000000000000404': 404, '0000000000000405': 405, '0000000000000406': 406, '0000000000000407': 407, '0000000000000408': 408, '0000000000000409': 409, '0000000000000410': 410, '0000000000000411': 411, '0000000000000412': 412, '0000000000000413': 413, '0000000000000414': 414, '0000000000000415': 415, '0000000000000416': 416, '0000000000000417': 417, '0000000000000418': 418, '0000000000000419': 419, '0000000000000420': 420, '0000000000000421': 421, '0000000000000422': 422, '0000000000000423': 423, '0000000000000424': 424, '0000000000000425': 425, '0000000000000426': 426, '0000000000000427': 427, '0000000000000428': 428, '0000000000000429': 429, '0000000000000430': 430, '0000000000000431': 431, '0000000000000432': 432, '0000000000000433': 433, '0000000000000434': 434, '0000000000000435': 435, '0000000000000436': 436, '0000000000000437': 437, '0000000000000438': 438, '0000000000000439': 439, '0000000000000440': 440, '0000000000000441': 441, '0000000000000442': 442, '0000000000000443': 443, '0000000000000444': 444, '0000000000000445': 445, '0000000000000446': 446, '0000000000000447': 447, '0000000000000448': 448, '0000000000000449': 449, '0000000000000450': 450, '0000000000000451': 451, '0000000000000452': 452, '0000000000000453': 453, '0000000000000454': 454, '0000000000000455': 455, '0000000000000456': 456, '0000000000000457': 457, '0000000000000458': 458, '0000000000000459': 459, '0000000000000460': 460, '0000000000000461': 461, '0000000000000462': 462, '0000000000000463': 463, '0000000000000464': 464, '0000000000000465': 465, '0000000000000466': 466, '0000000000000467': 467, '0000000000000468': 468, '0000000000000469': 469, '0000000000000470': 470, '0000000000000471': 471, '0000000000000472': 472, '0000000000000473': 473, '0000000000000474': 474, '0000000000000475': 475, '0000000000000476': 476, '0000000000000477': 477, '0000000000000478': 478, '0000000000000479': 479, '0000000000000480': 480, '0000000000000481': 481, '0000000000000482': 482, '0000000000000483': 483, '0000000000000484': 484, '0000000000000485': 485, '0000000000000486': 486, '0000000000000487': 487, '0000000000000488': 488, '0000000000000489': 489, '0000000000000490': 490, '0000000000000491': 491, '0000000000000492': 492, '0000000000000493': 493, '0000000000000494': 494, '0000000000000495': 495, '0000000000000496': 496, '0000000000000497': 497, '0000000000000498': 498, '0000000000000499': 499, '0000000000000500': 500, '0000000000000501': 501, '0000000000000502': 502, '0000000000000503': 503, '0000000000000504': 504, '0000000000000505': 505, '0000000000000506': 506, '0000000000000507': 507, '0000000000000508': 508, '0000000000000509': 509, '0000000000000510': 510, '0000000000000511': 511, '0000000000000512': 512, '0000000000000513': 513, '0000000000000514': 514, '0000000000000515': 515, '0000000000000516': 516, '0000000000000517': 517, '0000000000000518': 518, '0000000000000519': 519, '0000000000000520': 520, '0000000000000521': 521, '0000000000000522': 522, '0000000000000523': 523, '0000000000000524': 524, '0000000000000525': 525, '0000000000000526': 526, '0000000000000527': 527, '0000000000000528': 528, '0000000000000529': 529, '0000000000000530': 530, '0000000000000531': 531, '0000000000000532': 532, '0000000000000533': 533, '0000000000000534': 534, '0000000000000535': 535, '0000000000000536': 536, '0000000000000537': 537, '0000000000000538': 538, '0000000000000539': 539, '0000000000000540': 540, '0000000000000541': 541, '0000000000000542': 542, '0000000000000543': 543, '0000000000000544': 544, '0000000000000545': 545, '0000000000000546': 546, '0000000000000547': 547, '0000000000000548': 548, '0000000000000549': 549, '0000000000000550': 550, '0000000000000551': 551, '0000000000000552': 552, '0000000000000553': 553, '0000000000000554': 554, '0000000000000555': 555, '0000000000000556': 556, '0000000000000557': 557, '0000000000000558': 558, '0000000000000559': 559, '0000000000000560': 560, '0000000000000561': 561, '0000000000000562': 562, '0000000000000563': 563, '0000000000000564': 564, '0000000000000565': 565, '0000000000000566': 566, '0000000000000567': 567, '0000000000000568': 568, '0000000000000569': 569, '0000000000000570': 570, '0000000000000571': 571, '0000000000000572': 572, '0000000000000573': 573, '0000000000000574': 574, '0000000000000575': 575, '0000000000000576': 576, '0000000000000577': 577, '0000000000000578': 578, '0000000000000579': 579, '0000000000000580': 580, '0000000000000581': 581, '0000000000000582': 582, '0000000000000583': 583, '0000000000000584': 584, '0000000000000585': 585, '0000000000000586': 586, '0000000000000587': 587, '0000000000000588': 588, '0000000000000589': 589, '0000000000000590': 590, '0000000000000591': 591, '0000000000000592': 592, '0000000000000593': 593, '0000000000000594': 594, '0000000000000595': 595, '0000000000000596': 596, '0000000000000597': 597, '0000000000000598': 598, '0000000000000599': 599, '0000000000000600': 600, '0000000000000601': 601, '0000000000000602': 602, '0000000000000603': 603, '0000000000000604': 604, '0000000000000605': 605, '0000000000000606': 606, '0000000000000607': 607, '0000000000000608': 608, '0000000000000609': 609, '0000000000000610': 610, '0000000000000611': 611, '0000000000000612': 612, '0000000000000613': 613, '0000000000000614': 614, '0000000000000615': 615, '0000000000000616': 616, '0000000000000617': 617, '0000000000000618': 618, '0000000000000619': 619, '0000000000000620': 620, '0000000000000621': 621, '0000000000000622': 622, '0000000000000623': 623, '0000000000000624': 624, '0000000000000625': 625, '0000000000000626': 626, '0000000000000627': 627, '0000000000000628': 628, '0000000000000629': 629, '0000000000000630': 630, '0000000000000631': 631, '0000000000000632': 632, '0000000000000633': 633, '0000000000000634': 634, '0000000000000635': 635, '0000000000000636': 636, '0000000000000637': 637, '0000000000000638': 638, '0000000000000639': 639, '0000000000000640': 640, '0000000000000641': 641, '0000000000000642': 642, '0000000000000643': 643, '0000000000000644': 644, '0000000000000645': 645, '0000000000000646': 646, '0000000000000647': 647, '0000000000000648': 648, '0000000000000649': 649, '0000000000000650': 650, '0000000000000651': 651, '0000000000000652': 652, '0000000000000653': 653, '0000000000000654': 654, '0000000000000655': 655, '0000000000000656': 656, '0000000000000657': 657, '0000000000000658': 658, '0000000000000659': 659, '0000000000000660': 660, '0000000000000661': 661, '0000000000000662': 662, '0000000000000663': 663, '0000000000000664': 664, '0000000000000665': 665, '0000000000000666': 666, '0000000000000667': 667, '0000000000000668': 668, '0000000000000669': 669, '0000000000000670': 670, '0000000000000671': 671, '0000000000000672': 672, '0000000000000673': 673, '0000000000000674': 674, '0000000000000675': 675, '0000000000000676': 676, '0000000000000677': 677, '0000000000000678': 678, '0000000000000679': 679, '0000000000000680': 680, '0000000000000681': 681, '0000000000000682': 682, '0000000000000683': 683, '0000000000000684': 684, '0000000000000685': 685, '0000000000000686': 686, '0000000000000687': 687, '0000000000000688': 688, '0000000000000689': 689, '0000000000000690': 690, '0000000000000691': 691, '0000000000000692': 692, '0000000000000693': 693, '0000000000000694': 694, '0000000000000695': 695, '0000000000000696': 696, '0000000000000697': 697, '0000000000000698': 698, '0000000000000699': 699, '0000000000000700': 700, '0000000000000701': 701, '0000000000000702': 702, '0000000000000703': 703, '0000000000000704': 704, '0000000000000705': 705, '0000000000000706': 706, '0000000000000707': 707, '0000000000000708': 708, '0000000000000709': 709, '0000000000000710': 710, '0000000000000711': 711, '0000000000000712': 712, '0000000000000713': 713, '0000000000000714': 714, '0000000000000715': 715, '0000000000000716': 716, '0000000000000717': 717, '0000000000000718': 718, '0000000000000719': 719, '0000000000000720': 720, '0000000000000721': 721, '0000000000000722': 722, '0000000000000723': 723, '0000000000000724': 724, '0000000000000725': 725, '0000000000000726': 726, '0000000000000727': 727, '0000000000000728': 728, '0000000000000729': 729, '0000000000000730': 730, '0000000000000731': 731, '0000000000000732': 732, '0000000000000733': 733, '0000000000000734': 734, '0000000000000735': 735, '0000000000000736': 736, '0000000000000737': 737, '0000000000000738': 738, '0000000000000739': 739, '0000000000000740': 740, '0000000000000741': 741, '0000000000000742': 742, '0000000000000743': 743, '0000000000000744': 744, '0000000000000745': 745, '0000000000000746': 746, '0000000000000747': 747, '0000000000000748': 748, '0000000000000749': 749, '0000000000000750': 750, '0000000000000751': 751, '0000000000000752': 752, '0000000000000753': 753, '0000000000000754': 754, '0000000000000755': 755, '0000000000000756': 756, '0000000000000757': 757, '0000000000000758': 758, '0000000000000759': 759, '0000000000000760': 760, '0000000000000761': 761, '0000000000000762': 762, '0000000000000763': 763, '0000000000000764': 764, '0000000000000765': 765, '0000000000000766': 766, '0000000000000767': 767, '0000000000000768': 768, '0000000000000769': 769, '0000000000000770': 770, '0000000000000771': 771, '0000000000000772': 772, '0000000000000773': 773, '0000000000000774': 774, '0000000000000775': 775, '0000000000000776': 776, '0000000000000777': 777, '0000000000000778': 778, '0000000000000779': 779, '0000000000000780': 780, '0000000000000781': 781, '0000000000000782': 782, '0000000000000783': 783, '0000000000000784': 784, '0000000000000785': 785, '0000000000000786': 786, '0000000000000787': 787, '0000000000000788': 788, '0000000000000789': 789, '0000000000000790': 790, '0000000000000791': 791, '0000000000000792': 792, '0000000000000793': 793, '0000000000000794': 794, '0000000000000795': 795, '0000000000000796': 796, '0000000000000797': 797, '0000000000000798': 798, '0000000000000799': 799, '0000000000000800': 800, '0000000000000801': 801, '0000000000000802': 802, '0000000000000803': 803, '0000000000000804': 804, '0000000000000805': 805, '0000000000000806': 806, '0000000000000807': 807, '0000000000000808': 808, '0000000000000809': 809, '0000000000000810': 810, '0000000000000811': 811, '0000000000000812': 812, '0000000000000813': 813, '0000000000000814': 814, '0000000000000815': 815, '0000000000000816': 816, '0000000000000817': 817, '0000000000000818': 818, '0000000000000819': 819, '0000000000000820': 820, '0000000000000821': 821, '0000000000000822': 822, '0000000000000823': 823, '0000000000000824': 824, '0000000000000825': 825, '0000000000000826': 826, '0000000000000827': 827, '0000000000000828': 828, '0000000000000829': 829, '0000000000000830': 830, '0000000000000831': 831, '0000000000000832': 832, '0000000000000833': 833, '0000000000000834': 834, '0000000000000835': 835, '0000000000000836': 836, '0000000000000837': 837, '0000000000000838': 838, '0000000000000839': 839, '0000000000000840': 840, '0000000000000841': 841, '0000000000000842': 842, '0000000000000843': 843, '0000000000000844': 844, '0000000000000845': 845, '0000000000000846': 846, '0000000000000847': 847, '0000000000000848': 848, '0000000000000849': 849, '0000000000000850': 850, '0000000000000851': 851, '0000000000000852': 852, '0000000000000853': 853, '0000000000000854': 854, '0000000000000855': 855, '0000000000000856': 856, '0000000000000857': 857, '0000000000000858': 858, '0000000000000859': 859, '0000000000000860': 860, '0000000000000861': 861, '0000000000000862': 862, '0000000000000863': 863, '0000000000000864': 864, '0000000000000865': 865, '0000000000000866': 866, '0000000000000867': 867, '0000000000000868': 868, '0000000000000869': 869, '0000000000000870': 870, '0000000000000871': 871, '0000000000000872': 872, '0000000000000873': 873, '0000000000000874': 874, '0000000000000875': 875, '0000000000000876': 876, '0000000000000877': 877, '0000000000000878': 878, '0000000000000879': 879, '0000000000000880': 880, '0000000000000881': 881, '0000000000000882': 882, '0000000000000883': 883, '0000000000000884': 884, '0000000000000885': 885, '0000000000000886': 886, '0000000000000887': 887, '0000000000000888': 888, '0000000000000889': 889, '0000000000000890': 890, '0000000000000891': 891, '0000000000000892': 892, '0000000000000893': 893, '0000000000000894': 894, '0000000000000895': 895, '0000000000000896': 896, '0000000000000897': 897, '0000000000000898': 898, '0000000000000899': 899, '0000000000000900': 900, '0000000000000901': 901, '0000000000000902': 902, '0000000000000903': 903, '0000000000000904': 904, '0000000000000905': 905, '0000000000000906': 906, '0000000000000907': 907, '0000000000000908': 908, '0000000000000909': 909, '0000000000000910': 910, '0000000000000911': 911, '0000000000000912': 912, '0000000000000913': 913, '0000000000000914': 914, '0000000000000915': 915, '0000000000000916': 916, '0000000000000917': 917, '0000000000000918': 918, '0000000000000919': 919, '0000000000000920': 920, '0000000000000921': 921, '0000000000000922': 922, '0000000000000923': 923, '0000000000000924': 924, '0000000000000925': 925, '0000000000000926': 926, '0000000000000927': 927, '0000000000000928': 928, '0000000000000929': 929, '0000000000000930': 930, '0000000000000931': 931, '0000000000000932': 932, '0000000000000933': 933, '0000000000000934': 934, '0000000000000935': 935, '0000000000000936': 936, '0000000000000937': 937, '0000000000000938': 938, '0000000000000939': 939, '0000000000000940': 940, '0000000000000941': 941, '0000000000000942': 942, '0000000000000943': 943, '0000000000000944': 944, '0000000000000945': 945, '0000000000000946': 946, '0000000000000947': 947, '0000000000000948': 948, '0000000000000949': 949, '0000000000000950': 950, '0000000000000951': 951, '0000000000000952': 952, '0000000000000953': 953, '0000000000000954': 954, '0000000000000955': 955, '0000000000000956': 956, '0000000000000957': 957, '0000000000000958': 958, '0000000000000959': 959, '0000000000000960': 960, '0000000000000961': 961, '0000000000000962': 962, '0000000000000963': 963, '0000000000000964': 964, '0000000000000965': 965, '0000000000000966': 966, '0000000000000967': 967, '0000000000000968': 968, '0000000000000969': 969, '0000000000000970': 970, '0000000000000971': 971, '0000000000000972': 972, '0000000000000973': 973, '0000000000000974': 974, '0000000000000975': 975, '0000000000000976': 976, '0000000000000977': 977, '0000000000000978': 978, '0000000000000979': 979, '0000000000000980': 980, '0000000000000981': 981, '0000000000000982': 982, '0000000000000983': 983, '0000000000000984': 984, '0000000000000985': 985, '0000000000000986': 986, '0000000000000987': 987, '0000000000000988': 988, '0000000000000989': 989, '0000000000000990': 990, '0000000000000991': 991, '0000000000000992': 992, '0000000000000993': 993, '0000000000000994': 994, '0000000000000995': 995, '0000000000000996': 996, '0000000000000997': 997, '0000000000000998': 998, '0000000000000999': 999}\n",
            "{'0knife': 0, '0spear': 1, '0sword': 2, 'dagger': 3}\n",
            "classes in directory doesn't match classes_num\n",
            "old true, new 0true\n",
            "{'0true': 0, 'false': 1}\n",
            "classes in directory doesn't match classes_num\n",
            "50000 0\n",
            "3386 847\n",
            "71 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBvuSHZ6_wdO"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxB4w5RUD0kw",
        "outputId": "f276b8f0-fd07-483a-f4f8-f13e4f29d206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Loss function\n",
        "def compactness_loss(y_true, y_pred):\n",
        "    n_dim = np.shape(y_pred)[0]  # number of features vecs\n",
        "    k_dim = np.shape(y_pred)[1]  # feature vec dim\n",
        "    lc = 1/(k_dim*n_dim)* n_dim**2 * K.sum((y_pred - K.mean(y_pred,axis=0))**2,axis=[1]) / ((n_dim-1)**2)\n",
        "    return lc\n",
        "\n",
        "#Learning\n",
        "def train(target_dataloader, reference_dataloader, epoch_num):\n",
        "    print(\"Model build...\")\n",
        "\n",
        "    network = network_constractor()\n",
        "    #Freeze weight\n",
        "    for layer in network.layers:\n",
        "        if layer.name == first_trained_layer_name:\n",
        "            break\n",
        "        else:\n",
        "            layer.trainable = False\n",
        "    # Secondary network\n",
        "    model_t = Model(inputs=network.input,outputs=network.layers[-2].output)\n",
        "    # Reference network\n",
        "    #Apply a Fully Connected Layer to R\n",
        "    # prediction = Dense(classes, activation='softmax')(model_t.output) # for fmnist\n",
        "    model_r = Model(inputs=model_t.input,outputs=network.layers[-1].output) # for imagenet\n",
        "    \n",
        "    #Compile\n",
        "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    optimizer = SGD(lr=5e-5, decay=0.00005)\n",
        "    model_r.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[train_accuracy])\n",
        "    model_t.compile(optimizer=optimizer, loss=compactness_loss)\n",
        "\n",
        "    # Prints run settings\n",
        "    model_t.summary()\n",
        "    model_r.summary()\n",
        "    print(\"x_target is\", len(target_dataloader),'samples')\n",
        "    print(\"x_ref is\",len(reference_dataloader),'samples')\n",
        "\n",
        "    # run loggers\n",
        "    epochs_writer = tf.summary.create_file_writer(logdir=epochs_log_dir)\n",
        "    # batchs_writer = tf.summary.create_file_writer(logdir=batchs_log_dir)\n",
        "\n",
        "    outputs = {\"d loss\": [], \"c loss\": [], \"accuracy\": []}\n",
        "    loss, loss_c, epoch_accuracy = [], [], []\n",
        "\n",
        "    print(\"training...\")\n",
        "    #Learning\n",
        "    for epochnumber in range(epoch_num):\n",
        "        lc, ld, accuracy = [], [], [] # epoch loaders\n",
        "        for i in range(int(len(target_dataloader) / batchsize)):\n",
        "            #Load data for batch size \n",
        "            batch_is_ready = False\n",
        "            while not batch_is_ready:\n",
        "              try:\n",
        "                batch_target, _ = target_dataloader.next()\n",
        "                batch_is_ready = True\n",
        "              except: # some error in loading the data\n",
        "                if not target_dataloader.has_next():\n",
        "                    target_dataloader.on_epoch_end()\n",
        "                continue\n",
        "\n",
        "            batch_ref, batch_y = reference_dataloader.next()\n",
        "\n",
        "            #target data\n",
        "            #Get loss while learning\n",
        "            lc.append(model_t.train_on_batch(batch_target, np.zeros((batchsize, 4096))))\n",
        "\n",
        "            #reference data\n",
        "            #Get loss while learning\n",
        "            ref_output = model_r.train_on_batch(batch_ref, batch_y)\n",
        "            ld.append(ref_output[0])\n",
        "            accuracy.append(ref_output[1])\n",
        "            count = (epochnumber * int(len(target_dataloader) / batchsize)) + i\n",
        "\n",
        "            # if count % 50 == 0:\n",
        "            #   with batchs_writer.as_default():\n",
        "            #     tf.summary.scalar(\"d loss\", np.mean(ld), step=count)\n",
        "            #     tf.summary.scalar(\"c loss\", np.mean(lc), step=count)\n",
        "            #     tf.summary.scalar(\"accuracy\", np.mean(accuracy), step=count)\n",
        "            #   print(\"batch: {}: d loss {}, c loss {}, accuracy {}\".format(count, np.mean(ld), np.mean(lc), np.mean(accuracy)))\n",
        "\n",
        "        target_dataloader.on_epoch_end()\n",
        "        reference_dataloader.on_epoch_end()\n",
        "\n",
        "        outputs[\"d loss\"].append(np.mean(ld))\n",
        "        outputs[\"c loss\"].append(np.mean(lc))\n",
        "        outputs[\"accuracy\"].append(np.mean(accuracy))\n",
        "        with epochs_writer.as_default():\n",
        "          for key in outputs:\n",
        "            tf.summary.scalar(key, outputs[key][-1], step=epochnumber+1)\n",
        "        print(\"epoch: {}: d loss {}, c loss {}, accuracy {}\".format(epochnumber+1, outputs[\"d loss\"][-1], outputs[\"c loss\"][-1], outputs[\"accuracy\"][-1] ))\n",
        "\n",
        "        checkpoint_path = \"weights_after_{}_epochs\".format(epochnumber+1)\n",
        "        network.save_weights(os.path.join(ckpt_path, checkpoint_path))\n",
        "\n",
        "\n",
        "\n",
        "    #Result graph\n",
        "    plt.plot(outputs[\"d loss\"],label=\"Descriptive loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(outputs[\"c loss\"],label=\"Compact loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend()\n",
        "    plt.show() \n",
        "\n",
        "    plt.plot(outputs[\"accuracy\"],label=\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend()\n",
        "    plt.show()   \n",
        "\n",
        "    return model_t\n",
        "\n",
        "\n",
        "\n",
        "model = train(train_s_loader, ref_loader, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model build...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 14s 0us/step\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 126,625,280\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 130,722,280\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n",
            "x_target is 3386 samples\n",
            "x_ref is 50000 samples\n",
            "training...\n",
            "epoch: 1: d loss 1.460319324165379, c loss 0.38151537905553357, accuracy 0.6582988777318369\n",
            "epoch: 2: d loss 1.491405485421256, c loss 0.23264440009893042, accuracy 0.651801535735381\n",
            "epoch: 3: d loss 1.4192389678965969, c loss 0.1943269023848879, accuracy 0.6535735380980507\n",
            "epoch: 4: d loss 1.4387270324997177, c loss 0.17160108383817568, accuracy 0.6618428824571766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d039a59553ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_s_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-d039a59553ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(target_dataloader, reference_dataloader, epoch_num)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mbatch_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m#target data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-713a5e8d564a>\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelevant_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-713a5e8d564a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelevant_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelevant_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-713a5e8d564a>\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'RGB'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2818\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ZEzDL00OVX"
      },
      "source": [
        "### **Roc Curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNRAOOuih_Mp"
      },
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def get_data_scores(model, tempates, data_batch):\n",
        "  templates_preds = model.predict(tempates)\n",
        "  test_preds = model.predict(data_batch)\n",
        "\n",
        "  templates_preds = templates_preds.reshape((len(tempates),-1))\n",
        "  test_preds = test_preds.reshape((len(data_batch),-1))\n",
        "\n",
        "  test_s_losses = np.zeros(len(test_preds))\n",
        "  for i in range(len(test_preds)):\n",
        "    losses = tf.keras.losses.mean_squared_error(templates_preds, tf.expand_dims(test_preds[i], axis=0))\n",
        "    test_s_losses[i] = np.array(tf.math.reduce_min(losses))\n",
        "\n",
        "  return test_s_losses\n",
        "\n",
        "\n",
        "def get_roc_curve(model, templates, negative_data, positive_data, output_path):\n",
        "  roc_path = os.path.join(output_path, \"roc_graphs\")\n",
        "\n",
        "  if not os.path.exists(roc_path):\n",
        "    os.makedirs(roc_path)\n",
        "  #Abnormal score\n",
        "  Z1 = get_data_scores(model, templates, negative_data)\n",
        "  Z2 = get_data_scores(model, templates, positive_data)\n",
        "\n",
        "    #Drawing of ROC curve\n",
        "  y_true = np.zeros(len(negative_data)+len(positive_data))\n",
        "  y_true[len(negative_data):] = 1 #0:Normal, 1：Abnormal\n",
        "\n",
        "  #Calculate FPR, TPR(, Threshould)\n",
        "  fpr, tpr, _ = metrics.roc_curve(y_true, np.hstack((Z1, Z2)))\n",
        "\n",
        "  #AUC\n",
        "  auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "  #Plot the ROC curve\n",
        "  plt.figure()\n",
        "  plt.plot(fpr, tpr, label='DeepOneClassification(AUC = %.2f)'%auc)\n",
        "  plt.legend()\n",
        "  plt.title('ROC curve - after {} epochs'.format(EPOCH_NUM))\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.grid(True)\n",
        "  plt.savefig(os.path.join(roc_path, \"roc_graph.png\"))\n",
        "  plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LXMv-oalt0n"
      },
      "source": [
        "### **Scores Graphs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpIuD54500b2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FLNGMf_ECQH"
      },
      "source": [
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "import numpy as np\n",
        "import math\n",
        "from skimage.transform import resize\n",
        "import os\n",
        "\n",
        "\n",
        "def map_path_to_class(paths):\n",
        "  paths2cls = dict()\n",
        "  for path in paths:\n",
        "    cls = path.split('/')[-2]\n",
        "    paths2cls[path] = cls\n",
        "  return paths2cls\n",
        "\n",
        "\n",
        "def getImage(path, zoom):\n",
        "  image = plt.imread(path)\n",
        "  image = resize(image, (224, 224))\n",
        "  return OffsetImage(image, zoom=zoom)\n",
        "\n",
        "\n",
        "def create_images_graph(output_path, paths, scores, name, zoom, columns, max_objects=None):\n",
        "  scores_graph_path = os.path.join(output_path, \"score_graphs\")\n",
        "  if not os.path.exists(scores_graph_path):\n",
        "    os.makedirs(scores_graph_path)\n",
        "  if max_objects is None:\n",
        "    max_objects = len(scores)\n",
        "  paths2cls = map_path_to_class(paths)\n",
        "  indices =  np.argsort(scores)[:max_objects]\n",
        "  scores = scores[indices]\n",
        "\n",
        "  step = 10\n",
        "\n",
        "  x = list(range(columns)) * math.ceil(len(indices)/float(columns))\n",
        "\n",
        "  x = [step * i for i in x]\n",
        "  x = x[:len(scores)]\n",
        "  fig, ax = plt.subplots()\n",
        "  # ax.scatter(x, scores[indices]) \n",
        "  for i in range(max_objects):\n",
        "    idx = indices[i]\n",
        "    ab = AnnotationBbox(getImage(paths[idx], zoom), (x[i], scores[i]), frameon=False)\n",
        "    ax.scatter(x[i], scores[i]) \n",
        "    ax.add_artist(ab)\n",
        "  ax.update_datalim(np.column_stack([x, scores]))\n",
        "  ax.autoscale(-1*max(scores),max(scores)*1.1)\n",
        "  ax.set_xlim(-1,max(x)*1.1)\n",
        "  plt.ylabel(\"classifier score\")\n",
        "  plt.xlabel(\"axis without meaning\")\n",
        "  plt.title(name)\n",
        "  plt.savefig(os.path.join(scores_graph_path, \"scores visualization_of_{}.png\".format(name)), dpi=500)\n",
        "  plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2JfrBJYqTyZ"
      },
      "source": [
        "### **Features Graphs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-bj0yDpkLga"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def get_features_graph(model, templates, target_data, alien_data, output_path):\n",
        "  templates_preds = model.predict(templates)\n",
        "  target_preds = model.predict(target_data)\n",
        "  alien_preds = model.predict(alien_data)\n",
        "\n",
        "  templates_embedded = TSNE(n_components=2, random_state=123).fit_transform(templates_preds)\n",
        "  target_embedded = TSNE(n_components=2, random_state=123).fit_transform(target_preds)\n",
        "  alien_embedded = TSNE(n_components=2, random_state=123).fit_transform(alien_preds)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.scatter(templates_embedded[:, 0],templates_embedded[:,1], label=\"Templates\")\n",
        "  plt.scatter(target_embedded[:, 0],target_embedded[:,1], label=\"Target\")\n",
        "  plt.scatter(alien_embedded[:, 0],alien_embedded[:,1], label=\"Alien\")\n",
        "  plt.title(\"features graph (TSNE)\")\n",
        "  plt.savefig(os.path.join(output_path, \"features_graphs.png\"))\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28UFdpMqatL"
      },
      "source": [
        "### **Grad-CAM visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMYIE7v5lj9h"
      },
      "source": [
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, templates):\n",
        "        # store the model, the class index used to measure the class\n",
        "        # activation map, and the layer to be used when visualizing\n",
        "        # the class activation map\n",
        "        self.model = Model(inputs=model.input, outputs=model.layers[-1].output)\n",
        "        self.templates = templates\n",
        "        self.layerName = self.find_target_layer()\n",
        "\n",
        "    def find_target_layer(self):\n",
        "        # attempt to find the final convolutional layer in the network\n",
        "        # by looping over the layers of the network in reverse order\n",
        "        for layer in reversed(self.model.layers):\n",
        "            # check to see if the layer has a 4D output\n",
        "            if len(layer.output_shape) == 4:\n",
        "                return layer.name\n",
        "        # otherwise, we could not find a 4D layer so the GradCAM\n",
        "        # algorithm cannot be applied\n",
        "        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n",
        "\n",
        "\n",
        "\n",
        "    def compute_heatmap(self, image,eps=1e-8):\n",
        "        # construct our gradient model by supplying (1) the inputs\n",
        "        # to our pre-trained model, (2) the output of the (presumably)\n",
        "        # final 4D layer in the network, and (3) the output of the\n",
        "        # softmax activations from the model\n",
        "        gradModel = Model(\n",
        "            inputs=[self.model.inputs],\n",
        "            outputs=[self.model.get_layer(self.layerName).output,\n",
        "                     self.model.output])\n",
        "        # record operations for automatic differentiation\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # cast the image tensor to a float-32 data type, pass the\n",
        "            # image through the gradient model, and grab the loss\n",
        "            # associated with the specific class index\n",
        "            inputs = tf.cast(image, tf.float32)\n",
        "            inputs = tf.Variable(inputs)\n",
        "            tape.watch(inputs)\n",
        "\n",
        "            (convOutputs, predictions) = gradModel(inputs)\n",
        "            (t_convOutputs, t_predictions) = gradModel(self.templates)\n",
        "            train = tf.reshape(t_predictions, (len(self.templates), -1))\n",
        "            test = tf.reshape(predictions, (len(image), -1))\n",
        "            losses = tf.keras.losses.mean_squared_error(train, test)\n",
        "\n",
        "            loss = tf.math.reduce_min(losses)\n",
        "\n",
        "        # use automatic differentiation to compute the gradients\n",
        "        grads = tape.gradient(loss, convOutputs)\n",
        "        # compute the guided gradients\n",
        "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
        "        castGrads = tf.cast(grads > 0, \"float32\")\n",
        "        guidedGrads = castConvOutputs * castGrads * grads\n",
        "        # the convolution and guided gradients have a batch dimension\n",
        "        # (which we don't need) so let's grab the volume itself and\n",
        "        # discard the batch\n",
        "        convOutputs = convOutputs[0]\n",
        "        guidedGrads = guidedGrads[0]\n",
        "\n",
        "        # compute the average of the gradient values, and using them\n",
        "        # as weights, compute the ponderation of the filters with\n",
        "        # respect to the weights\n",
        "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
        "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
        "        # grab the spatial dimensions of the input image and resize\n",
        "        # the output class activation map to match the input image\n",
        "        # dimensions\n",
        "        (w, h) = (image.shape[2], image.shape[1])\n",
        "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
        "        # normalize the heatmap such that all values lie in the range\n",
        "        # [0, 1], scale the resulting values to the range [0, 255],\n",
        "        # and then convert to an unsigned 8-bit integer\n",
        "        numer = heatmap - np.min(heatmap)\n",
        "        denom = (heatmap.max() - heatmap.min()) + eps\n",
        "        heatmap = numer / denom\n",
        "        heatmap = (heatmap * 255).astype(\"uint8\")\n",
        "        # return the resulting heatmap to the calling function\n",
        "        return heatmap\n",
        "\n",
        "    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n",
        "                        colormap=cv2.COLORMAP_JET ):\n",
        "        # apply the supplied color map to the heatmap and then\n",
        "        # overlay the heatmap on the input image\n",
        "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
        "        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
        "        # return a 2-tuple of the color mapped heatmap and the output,\n",
        "        # overlaid image\n",
        "        return (heatmap, output)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_grJNkybfin_"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "import imutils\n",
        "import re\n",
        "\n",
        "\n",
        "def image_name(image_path):\n",
        "  try:\n",
        "    regex = \".*[\\\\/|\\\\\\](.*)[\\\\/|\\\\\\](.*).(jpg|JPEG)\"\n",
        "    m = re.match(regex, image_path)\n",
        "    return m.group(1) + \"_\" + m.group(2)\n",
        "  except:\n",
        "    print(image_path)\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_gradCam_image(models_dict, image, image_path, output_path, templates, preprocess_func, loss_norm_dict):\n",
        "\n",
        "    # image = read_image(image_path, (size, size))\n",
        "    orig = np.array(Image.open(image_path).convert('RGB'))\n",
        "\n",
        "    all_outputs = []\n",
        "    labels_str = \"\"\n",
        "\n",
        "    losses_dict = dict()\n",
        "\n",
        "\n",
        "    for model_name in models_dict:\n",
        "      model = models_dict[model_name]\n",
        "      cam = GradCAM(model, templates)\n",
        "\n",
        "\n",
        "\n",
        "      loss = get_data_scores(model, templates, image)\n",
        "      losses_dict[model_name] = np.float(loss)\n",
        "\n",
        "      heatmap = cam.compute_heatmap(np.copy(image))\n",
        "      # heatmap = cv2.resize(heatmap, (image.shape[2], image.shape[1]))\n",
        "      # heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "      # heatmap = (heatmap * 255).astype(\"uint8\")\n",
        "\n",
        "\n",
        "\n",
        "      heatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))\n",
        "      (heatmap, output) = cam.overlay_heatmap(heatmap, orig, alpha=0.5)\n",
        "      all_outputs.append(np.hstack([orig, heatmap, output]))\n",
        "\n",
        "      # print(heatmap.shape)\n",
        "      # print(image[0].shape)\n",
        "\n",
        "\n",
        "      # all_outputs.append(np.hstack([image[0], heatmap]))\n",
        "      loss_norm = loss_norm_dict[model_name]\n",
        "      # print(loss, loss/loss_norm)\n",
        "      labels_str += \"model {}: loss {}\\n\".format(model_name, loss/loss_norm)\n",
        "    output = np.vstack(all_outputs)\n",
        "    output = imutils.resize(output, height=2100)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(output)\n",
        "    plt.title(labels_str)\n",
        "    plt.savefig(os.path.join(output_path, image_name(image_path)), bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    return losses_dict\n",
        "\n",
        "\n",
        "# def get_gradCam_image(models_dict, image_path, output_path, templates, preprocess_func, loss_norm_dict):\n",
        "\n",
        "#     image = read_image(image_path, (size, size))\n",
        "#     orig = np.array(Image.open(image_path).convert('RGB'))\n",
        "\n",
        "#     all_outputs = []\n",
        "#     labels_str = \"\"\n",
        "\n",
        "#     losses_dict = dict()\n",
        "\n",
        "\n",
        "#     for model_name in models_dict:\n",
        "#       model = models_dict[model_name]\n",
        "#       cam = GradCAM(model, templates)\n",
        "\n",
        "#       loss = get_data_scores(model, templates, image)\n",
        "#       losses_dict[model_name] = int(loss)\n",
        "\n",
        "#       heatmap = cam.compute_heatmap(preprocess_func(np.copy(image)))\n",
        "#       heatmap = cv2.resize(heatmap, (orig.shape[1], orig.shape[0]))\n",
        "#       (heatmap, output) = cam.overlay_heatmap(heatmap, orig, alpha=0.5)\n",
        "#       all_outputs.append(np.hstack([orig, heatmap, output]))\n",
        "#       loss_norm = loss_norm_dict[model_name]\n",
        "#       # print(loss, loss/loss_norm)\n",
        "#       labels_str += \"model {}: loss {}\\n\".format(model_name, loss)\n",
        "#     output = np.vstack(all_outputs)\n",
        "#     output = imutils.resize(output, height=2100)\n",
        "\n",
        "#     fig = plt.figure()\n",
        "#     plt.imshow(output)\n",
        "#     plt.title(labels_str)\n",
        "#     plt.savefig(os.path.join(output_path, image_name(image_path)), bbox_inches='tight')\n",
        "#     plt.close(fig)\n",
        "#     return losses_dict\n",
        "\n",
        "# def read_image(image_path, input_size):\n",
        "#     image = load_img(image_path, target_size=input_size)\n",
        "#     image = img_to_array(image)\n",
        "#     image = np.expand_dims(image, axis=0)\n",
        "#     image = imagenet_utils.preprocess_input(image)\n",
        "#     return image\n",
        "\n",
        "\n",
        "def read_image(image_path, input_size):\n",
        "    image = Image.open(image_path, 'r')\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    image = image.resize(input_size, Image.NEAREST)\n",
        "    image = np.array(image).astype(np.float32)\n",
        "    return np.expand_dims(image, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "def get_results_for_imagesdir(models_dict, images, paths, output_path, templates, preprocess_func, loss_norm_dict):\n",
        "\n",
        "  models_losses = dict()\n",
        "  for model_name in models_dict:\n",
        "    models_losses[model_name] = []\n",
        "\n",
        "  for i in range(len(paths)):\n",
        "    image = np.expand_dims(images[i], axis=0)\n",
        "    losses_dict = get_gradCam_image(models_dict, image, paths[i], output_path, templates, preprocess_func, loss_norm_dict)\n",
        "    for model_name in models_dict:\n",
        "      models_losses[model_name].append(losses_dict[model_name])\n",
        "  return models_losses\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px9U2lrZJf1U"
      },
      "source": [
        "Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lfunWuFJjIs",
        "outputId": "edab9600-f787-43f9-89ce-7bfe5bb93ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# NAME= \"experiment_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "NAME = \"experiment_test\"\n",
        "OUTPUT_PATH =  os.path.join(\"/content/drive/My Drive/Colab Notebooks/affordances/experiments\", NAME)\n",
        "\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "  os.makedirs(OUTPUT_PATH)\n",
        "\n",
        "size = 224\n",
        "classes_num = 1000\n",
        "preprocessing_func = vgg_preprocessing\n",
        "network_constractor = lambda : tf.keras.applications.VGG16(include_top=True, input_shape=(size, size, 3), weights='imagenet')\n",
        "\n",
        "\n",
        "# creates_relevant_models\n",
        "EPOCH_NUM = 5\n",
        "models = dict()\n",
        "\n",
        "## model with all the stabbing data\n",
        "all_stab_model = network_constractor()\n",
        "all_stab_ckpt_path = os.path.join(\"/content/drive/My Drive/Colab Notebooks/affordances/experiments/experiment_unclean_target_all_stab/20201008-054752\", \"ckpts\")\n",
        "all_stab_model.load_weights(os.path.join(all_stab_ckpt_path, \"weights_after_{}_epochs\".format(EPOCH_NUM))).expect_partial()\n",
        "all_stab_model = Model(inputs=all_stab_model.input,outputs=all_stab_model.layers[-2].output)\n",
        "models[\"all_stab_model\"]= all_stab_model\n",
        "\n",
        "## model with only knives data\n",
        "knives_only_model = network_constractor()\n",
        "knives_only_ckpt_path = os.path.join(\"/content/drive/My Drive/Colab Notebooks/affordances/experiments/experiment_unclean_target_only_knives/20201007-141227\", \"ckpts\")\n",
        "knives_only_model.load_weights(os.path.join(knives_only_ckpt_path, \"weights_after_{}_epochs\".format(EPOCH_NUM))).expect_partial()\n",
        "knives_only_model = Model(inputs=knives_only_model.input,outputs=knives_only_model.layers[-2].output)\n",
        "models[\"knives_only_model\"]= knives_only_model\n",
        "\n",
        "## model with imagenet weights\n",
        "untrainbed_model = network_constractor()\n",
        "untrainbed_model = Model(inputs=untrainbed_model.input,outputs=untrainbed_model.layers[-2].output)\n",
        "models[\"untrainbed_model\"]= untrainbed_model\n",
        "\n",
        "\n",
        "# Data loaders\n",
        "target_train_paths, target_train_labels = os.path.join(OUTPUT_PATH, \"target_train_paths.txt\"), os.path.join(OUTPUT_PATH, \"target_train_labels.txt\")\n",
        "target_test_paths, target_test_labels = os.path.join(OUTPUT_PATH, \"target_test_paths.txt\"), os.path.join(OUTPUT_PATH, \"target_test_labels.txt\")\n",
        "alien_test_paths, alien_test_labels = os.path.join(OUTPUT_PATH, \"alien_test_paths.txt\"), os.path.join(OUTPUT_PATH, \"alien_test_labels.txt\")\n",
        "\n",
        "train_s_loader = construct_with_files(target_train_paths, None, 2, (size,size), classes_num, False, preprocessing_func)\n",
        "test_s_loader = construct_with_files(target_test_paths, None, 2, (size,size), classes_num, False, preprocessing_func)\n",
        "test_b_loader = construct_with_files(alien_test_paths, None, 2, (size,size), classes_num, False, preprocessing_func)\n",
        "\n",
        "templates, templates_paths = train_s_loader.get_all_data(size=40)\n",
        "target_test, target_test_paths = test_s_loader.get_all_data(size=70)\n",
        "alien_test, alien_test_paths = test_b_loader.get_all_data(size=70)\n",
        "\n",
        "\n",
        "# Creates epoch dirs\n",
        "get_epoch_dir = lambda model_name, epoch: os.path.join(os.path.join(OUTPUT_PATH, model_name), \"epoch_{}\".format(epoch))\n",
        "for model_name in models:\n",
        "  model_dir = os.path.join(OUTPUT_PATH, model_name)\n",
        "  if not os.path.exists(get_epoch_dir(model_dir, EPOCH_NUM)):\n",
        "    os.makedirs(get_epoch_dir(model_dir, EPOCH_NUM))\n",
        "\n",
        "\n",
        "# Creates roc curve graph\n",
        "for model_name in models:\n",
        "  get_roc_curve(models[model_name], templates, target_test, alien_test, get_epoch_dir(model_name, EPOCH_NUM))\n",
        "\n",
        "\n",
        "norm_factors = dict()\n",
        "z2s = dict()\n",
        "\n",
        "# Creates scores graphs\n",
        "for model_name in models:\n",
        "  Z1 = get_data_scores(models[model_name], templates, target_test)\n",
        "  Z2 = get_data_scores(models[model_name], templates, alien_test)\n",
        "\n",
        "  norm_factors[model_name] = max(np.max(Z1), np.max(Z2))\n",
        "  z2s[model_name] = Z2\n",
        "  # Z1 /= norm_factors[model_name]\n",
        "  # Z2 /= norm_factors[model_name]\n",
        "  scores_graph_output_path = get_epoch_dir(model_name, EPOCH_NUM)\n",
        "  create_images_graph(scores_graph_output_path, target_test_paths[:40], Z1[:40], \"scores_for_knives_images\", 0.08, 20)  # displays the first 40's examples\n",
        "  create_images_graph(scores_graph_output_path, target_test_paths, Z1, \"the_smallest_scores_for_knives_images\", 0.08, 20, 20) # the 20's examples with the lowest score\n",
        "  create_images_graph(scores_graph_output_path, alien_test_paths[:40], Z2[:40], \"scores_for_alien_images\", 0.08, 20) # displays the first 40's examples\n",
        "  create_images_graph(scores_graph_output_path, alien_test_paths, Z2, \"the_smallest_scores_for_alien_images\", 0.05, 20, 20) # the 20's examples with the lowest score\n",
        "\n",
        "# for model_name in models:\n",
        "#   features_graph_output_path = get_epoch_dir(model_name, EPOCH_NUM)\n",
        "#   get_features_graph(models[model_name], templates, target_test, alien_test, features_graph_output_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "grad_cam_output_path = os.path.join(os.path.join(OUTPUT_PATH, \"grad_cam\"), \"epoch_\" + str(EPOCH_NUM))\n",
        "to_visulize_root_dir = \"/content/drive/My Drive/Colab Notebooks/affordances/datasets/clean_alien_visulization\" \n",
        "losses = get_results_for_imagesdir(models, alien_test, alien_test_paths, grad_cam_output_path, templates, preprocessing_func, norm_factors)\n",
        "\n",
        "# for model_name in models:\n",
        "#   print(z2s[model_name])\n",
        "#   print(losses[model_name])\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DataLossError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f6cf10811a93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mall_stab_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork_constractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mall_stab_ckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/affordances/experiments/experiment_unclean_target_all_stab/20201008-054752\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ckpts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mall_stab_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_stab_ckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weights_after_{}_epochs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mall_stab_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_stab_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_stab_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"all_stab_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mall_stab_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2180\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m         raise NotImplementedError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         options=options)\n\u001b[1;32m   1319\u001b[0m     base.CheckpointPosition(\n\u001b[0;32m-> 1320\u001b[0;31m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[1;32m   1321\u001b[0m     load_status = CheckpointLoadStatus(\n\u001b[1;32m   1322\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[0;34m(self, checkpoint_position)\u001b[0m\n\u001b[1;32m    912\u001b[0m     restore_ops.extend(\n\u001b[1;32m    913\u001b[0m         current_position.checkpoint.restore_saveables(\n\u001b[0;32m--> 914\u001b[0;31m             tensor_saveables, python_saveables))\n\u001b[0m\u001b[1;32m    915\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[0;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[1;32m    295\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[1;32m    296\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[0;32m--> 297\u001b[0;31m           validated_saveables).restore(self.save_path_tensor, self.options)\n\u001b[0m\u001b[1;32m    298\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    338\u001b[0m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_function_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m       \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_restore_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m           \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix, options)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       restored_tensors = io_ops.restore_v2(\n\u001b[0;32m--> 104\u001b[0;31m           file_prefix, tensor_names, tensor_slices, tensor_dtypes)\n\u001b[0m\u001b[1;32m    105\u001b[0m     structured_restored_tensors = nest.pack_sequence_as(\n\u001b[1;32m    106\u001b[0m         tensor_structure, restored_tensors)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mrestore_v2\u001b[0;34m(prefix, tensor_names, shape_and_slices, dtypes, name)\u001b[0m\n\u001b[1;32m   1510\u001b[0m       return restore_v2_eager_fallback(\n\u001b[1;32m   1511\u001b[0m           \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_and_slices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m           ctx=_ctx)\n\u001b[0m\u001b[1;32m   1513\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mrestore_v2_eager_fallback\u001b[0;34m(prefix, tensor_names, shape_and_slices, dtypes, name, ctx)\u001b[0m\n\u001b[1;32m   1548\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"dtypes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m   _result = _execute.execute(b\"RestoreV2\", len(dtypes), inputs=_inputs_flat,\n\u001b[0;32m-> 1550\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   1551\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m     _execute.record_gradient(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataLossError\u001b[0m: Checksum does not match: stored 3002743735 vs. calculated on the restored bytes 3935372916 [Op:RestoreV2]"
          ]
        }
      ]
    }
  ]
}